[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Sociology (PGSP11583)",
    "section": "",
    "text": "This is the course book we will be using for Computational Sociology (PGSP11583).\n\nprint(\"Computational Sociology\")\n\n[1] \"Computational Sociology\"\n\n\n\n\n\n\n\n\nThe book is a “live” document meaning I will be updating as we progress together through the course.\n\n\n\n\n\nThe course is structured of alternating weeks of substantive and technical instruction.\n\n\n\nWeek\nFocus\nCoding skills\nWorkflow skills\nWorksheet\n\n\n\n\n1\nIntro. Computational Social Science\nIntroductory exercises + RTC Workshop by Ugur Ozdemir\nDirectories, Git, and Github\n\n\n\n2\nFilter Bubbles and Echo Chambers\nR4DS chap.3 and R4DS chap.4\n\n\n\n\n3\nWeb tracking and Network Analysis\nR4DS chap.5\nMarkdown\nWorksheet 1\n\n\n4\nMisinformation and Fake News\nR4DS chap.7\n\n\n\n\n5\nLinked surveys and Social media data\nR4DS chap.11 and R4DS chap.12\nGit and Github\nWorksheet 2\n\n\n6\nPolarization and Radicalization\nR4DS chap.13 and R4DS chap. 14\n\n\n\n\n7\nOnline experiments and Bots\nR4DS chap.15 and R4DS chap. 16\nMarkdown\n\n\n\n8\nViolence and Protest\nR4DS chap.18 and R4DS chap. 19\n\n\n\n\n9\nSocial media tracking and Natural Language Processing\nR4DS chap.20 and R4DS chap. 21\nDatabase management systems\n\n\n\n10\nGuest lecture: TBD\n\n\n\n\n\n\n\n\n\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "course_overview.html",
    "href": "course_overview.html",
    "title": "Course Overview",
    "section": "",
    "text": "This below gives details of course structure, outcomes, and assessment."
  },
  {
    "objectID": "course_overview.html#learning-outcomes",
    "href": "course_overview.html#learning-outcomes",
    "title": "Course Overview",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nThis course will give students training in the use of various computational methods in the social sciences. The course will prepare students for dissertation work that uses digital trace data and/or computational methods and will provide hands-on training in the use of the R programming language and (some) Python.\nThe course will provide a venue for seminar discussion of examples using these methods in the empirical social sciences as well as lectures on the technical and/or statistical dimensions of their application."
  },
  {
    "objectID": "course_overview.html#course-structure",
    "href": "course_overview.html#course-structure",
    "title": "Course Overview",
    "section": "Course structure",
    "text": "Course structure\nWe will be using this online book for the ten-week course in “Computational Sociology” (PGSP11583). Each chapter contains the readings for that week. The book also includes worksheets with example code for how to conduct some of the text analysis techniques we discuss each week.\nEach week (with the partial exception of week 1), we will be discussing, alternately, the substantive and technical dimensions of published research in the empirical social sciences."
  },
  {
    "objectID": "course_overview.html#course-theme",
    "href": "course_overview.html#course-theme",
    "title": "Course Overview",
    "section": "Course theme",
    "text": "Course theme\nIn order to discipline the course, I have decided to focus on one theme that is currently making headlines: the political consequences of social media.\nWith this in mind, every two weeks we will be studying a different phenomenon that has gained media attention: e.g., echo chambers, misinformation, violence.\nOn alternate weeks, we will discuss how these phenomena have been studied—using both computational and non-computational methods. In the subsequent week, we will then go through some of the technical dimensions of the methods used in the papers we study."
  },
  {
    "objectID": "course_overview.html#a-provocation",
    "href": "course_overview.html#a-provocation",
    "title": "Course Overview",
    "section": "A Provocation",
    "text": "A Provocation\nUnlike other courses you may take, I’m going to be taking a particular position on a question each week during the lecture. I want to invite you to prove me wrong.\nI’ll be encouraging you to do this at our flash talks (see below) every other week after the lecture has finished. You’re also free to air your opinions on Twitter, tag me, and I’ll respond. Alternatively, you can write a blog post on a platform such as Medium, Substack, whatever you might choose.\nThe idea is for this to be an ongoing conversation, debate, and opportunity for us to hear each side out."
  },
  {
    "objectID": "course_overview.html#course-pre-preparation",
    "href": "course_overview.html#course-pre-preparation",
    "title": "Course Overview",
    "section": "Course pre-preparation",
    "text": "Course pre-preparation\nNOTE: Before the lecture in Week 2, students should complete two introductory R exercises.\n\nFirst, you should consult the introduction to file directories and Github here. This is crucial to properly logging and saving the work you produce.\nSecond, you should consult the worksheet here, which is an introduction to setting up and understanding the very basics of working in R. S\nThird, Ugur Ozdemir has provided a more comprehensive introductory R course for the Research Training Centre at the University of Edinburgh and you can follow the instructions here to access this."
  },
  {
    "objectID": "course_overview.html#reference-sources",
    "href": "course_overview.html#reference-sources",
    "title": "Course Overview",
    "section": "Reference sources",
    "text": "Reference sources\nThere are two main reference texts that will be of use during this course:\n\nWickham, Hadley and Garrett Grolemund. R for Data Science: https://r4ds.had.co.nz/\nSalganik, Matt. Bit by Bit: Social Research in the Digital Age: https://www.bitbybitbook.com/"
  },
  {
    "objectID": "course_overview.html#assessment",
    "href": "course_overview.html#assessment",
    "title": "Course Overview",
    "section": "Assessment",
    "text": "Assessment\n\nFortnightly worksheets\nEach fortnight, I will provide you with one worksheet that walks you through how to implement a different computational technique. At the end of these worksheets you will find a set of questions. You should buddy up with someone else in your class and go through these together.\nThis is called “pair programming” and there’s a reason we do this. Firstly, coding can be an isolating and difficult thing—it’s good to bring a friend along for the ride! Secondly, if there’s something you don’t know, maybe your buddy will. This saves you both time. Thirdly, your buddy can check your code as you write it, and vice versa. Again, this means both of you are working together to produce and check something as you go along.\nAt the subsequent week’s lecture, I will pick on a pair at random to answer each one of that worksheet’s questions (i.e., there is ~1/3 chance you’re going to get picked each week). I will ask you to walk us through your code. And remember: it’s also fine if you struggled and didn’t get to the end! If you encountered an obstacle, we can work through that together. All that matters to me is that you try.\n\n\nFortnightly flash talks\nOn the weeks where you are not going to be tasked with a coding assignment, you’re not off the hook… I will again be selecting a pair at random (the same as your coding pair) to talk respond to me on the subject of the the readings. I will pick a different pair for each reading (i.e., ~ 1/3 chance again).\nI will be taking a position in response to these readings (I won’t tell you which one) and I’d like you to try to respond (a bit of devil’s advocacy). This is good practice for critically thinking through each side of a question—and the empirical evidence for those respective positions.\n\n\nFinal assessment\nAssessment takes the form of one summative assessment. This will be a 4000 word essay that will take the form of a review, replication, and extension of a published article. More details will follow."
  },
  {
    "objectID": "terminal_and_git.html",
    "href": "terminal_and_git.html",
    "title": "Managing data and code",
    "section": "",
    "text": "The following introductory section is taken, in slightly adapted form, from Jae Yeon Kim’s “Computational Thinking for Social Scientists.” It is reproduced here for ease of access. To consult the full book, go to https://jaeyk.github.io/comp_thinking_social_science."
  },
  {
    "objectID": "terminal_and_git.html#the-command-line",
    "href": "terminal_and_git.html#the-command-line",
    "title": "Managing data and code",
    "section": "The Command Line",
    "text": "The Command Line\nWhat is the command line? The command line or “terminal” is what you’ll recognize from Hollywood portrayals of hackers and script kids—a black screen containing single lines of code, sometimes cascading down the page.\nBut you all have one on your computers.\nOn Windows computers, you’ll find this under the name “Command Prompt.” See this guide on how to open.\nOn Mac, this is called “Terminal”. See this guide. Some also prefer to use an application called “iTerm 2” but they both essentially do the same thing.\n\nThe Big Picture\nAs William Shotts the author of The Linux Command Line put it:\n\ngraphical user interfaces make easy tasks easy, while command-line interfaces make difficult tasks possible.\n\n\n\nWhy bother using the command line?\nSuppose that we want to create a plain text file that contains the word “test.” If we want to do this in the command line, you need to know the following commands.\n\necho: “Write arguments to the standard output” This is equivalent to using a text editor (e.g., nano, vim, emacs) and writing something.\n> test Save the expression in a file named test.\n\nWe can put these commands together like the following:\necho \"sth\" > test \nDon’t worry if you are worried about memorizing these and more commands. Memorization is a far less important aspect of learning programming. In general, if you don’t know what a command does, just type <command name> --help. You can do man <command name> to obtain further information. Here, man stands for manual. If you need more user-friendly information, please consider using tldr.\nLet’s make this simple case complex by scaling up. Suppose we want to make 100 duplicates of the test file. Below is the one-line code that performs the task!\nfor i in {1..100}; do cp test \"test_$i\"; done  \nLet me break down the seemingly complex workflow.\n\nfor i in {1..100}. This is a for loop. The numbers 1..100 inside the curly braces {} indicates the range of integers from 1 to 100. In R, this is equivalent to for (i in 1:100) {}\n\n; is used to use multiple commands without making line breaks. ; works in the same way in R.\n$var returns the value associated with a variable. Type name=<Your name>. Then, type echo $name. You should see your name printed. Variable assignment is one of the most basic things you’ll learn in any programming. In R, we do this by using ->\n\nIf you have zero experience in programming, I might have provided too many concepts too early, like variable assignment and for loop. However, you don’t need to worry about them at this point. We will cover them in the next chapter.\nI will give you one more example to illustrate how powerful the command line is. Suppose we want to find which file contains the character “COVID.” This is equivalent to finding a needle in a haystack. It’s a daunting task for humans, but not for computers. Commands are verbs. So, to express this problem in a language that computers could understand, let’s first find what command we should use. Often, a simple Google or Stack Overflow search leads to an answer.\nIn this case, grep is the answer (there’s also grep in R). This command finds PATTERNS in each FILE. What follows - are options (called flags): r (recursive), n (line number), w (match only whole words), e (use patterns for matching). rnw are for output control and e is for pattern selection.\nSo, to perform the task above, you just need one-line code: grep -r -n -w -e \"COVID''\nQuick reminders - grep: command - -rnw -e: flags - COVID: argument (usually file or file paths)\nLet’s remove (=rm) all the duplicate files and the original file. * (any number of characters) is a wildcard (if you want to identify a single number of characters, use ?). It finds every file whose name starts with test_.\nrm test_* test \nEnough with demonstrations. What is this black magic? Can you do the same thing using a graphical interface? Which method is more efficient? I hope that my demonstrations give you enough sense of why learning the command line could be incredibly useful. In my experience, mastering the command line helps automate your research process from end to end. For instance, you don’t need to write files from a website using your web browser. Instead, you can run the wget command in the terminal. Better yet, you don’t even need to run the command for the second time. You can write a Shell script (*.sh) that automates downloading, moving, and sorting multiple files.\n\n\nUNIX Shell\nThe other thing you might have noticed is that there are many overlaps between the commands and base R functions (R functions that can be used without installing additional packages). This connection is not coincident. UNIX preceded and influenced many programming languages, including R.\nThe following materials on UNIX and Shell are adapted from [the software carpentry](https://bids.GitHub.io/2015-06-04-berkeley/shell/00-intro.html.\n\nUnix\nUNIX is an operating system + a set of tools (utilities). It was developed by AT & T employees at Bell Labs (1969-1971). From Mac OS X to Linux, many of the current operation systems are some versions of UNIX. Command-line INTERFACE is a way to communicate with your OS by typing, not pointing, and clicking.\nFor this reason, if you’re using Mac OS, then you don’t need to do anything else to experience UNIX. You’re already all set.\nIf you’re using Windows, you need to install either GitBash (a good option if you only use Bash for Git and GitHub) or Windows Subsystem (highly recommended if your use case goes beyond Git and GitHub). For more information, see this installation guideline. If you’re a Windows user and don’t use Windows 10, I recommend installing VirtualBox.\nUNIX is old, but it is still mainstream, and it will be. Moreover, the UNIX philosophy (“Do One Thing And Do It Well”)—minimalist, modular software development—is highly and widely influential.\n\n\n  \nAT&T Archives: The UNIX Operating System\n\n\n\nUnix50 - Unix Today and Tomorrow: The Languages \n\n\nKernel\nThe kernel of UNIX is the hub of the operating system: it allocates time and memory to programs. It handles the filestore (e.g., files and directories) and communications in response to system calls.\n\n\nShell\nThe shell is an interactive program that provides an interface between the user and the kernel. The shell interprets commands entered by the user or supplied by a Shell script and passes them to the kernel for execution.\n\n\nHuman-Computer interfaces\nAt a high level, computers do four things:\n\nrun programs\nstore data\ncommunicate with each other\ninteract with us (through either CLI or GUI)\n\n\n\nThe Command Line\nThis kind of interface is called a command-line interface, or CLI, to distinguish it from the graphical user interface, or GUI, that most people now use.\nThe heart of a CLI is a read-evaluate-print loop, or REPL: when the user types a command and then presses the enter (or return) key, the computer reads it, executes it, and prints its output. The user then types another command, and so on until the user logs off.\nIf you’re using RStudio, you can use terminal inside RStudio (next to the “Console”). (For instance, type Alt + Shift + M)\n\n\nThe Shell\nThis description makes it sound as though the user sends commands directly to the computer and sends the output directly to the user. In fact, there is usually a program in between called a command shell.\n\n\n\nSource: Prashant Lakhera\n\n\nWhat the user types go into the shell; it figures out what commands to run and orders the computer to execute them.\nNote, the shell is called the shell: it encloses the operating system to hide some of its complexity and make it simpler to interact with.\nA shell is a program like any other. What’s special about it is that its job is to run other programs rather than do calculations itself. The commands are themselves programs: when they terminate, the shell gives the user another prompt ($ on our systems).\n\n\nBash\nThe most popular Unix shell is Bash, the Bourne Again Shell (so-called because it’s derived from a shell written by Stephen Bourne — this is what passes for wit among programmers). Bash is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows.\n\n\nWhy Shell?\nUsing Bash or any other shell sometimes feels more like programming than like using a mouse. Commands are terse (often only a couple of characters long), their names are frequently cryptic, and their output is lines of text rather than something visual like a graph.\nOn the other hand, the shell allows us to combine existing tools in powerful ways with only a few keystrokes and set up pipelines to handle large volumes of data automatically.\nIn addition, the command line is often the easiest way to interact with remote machines (explains why we learn Bash before learning Git and GitHub). If you work in a team and your team manages data in a remote server, you will likely need to get access the server via something like ssh.\n\n\nOur first command\nThe part of the operating system responsible for managing files and directories is called the file system. It organizes our data into files, which hold information, and directories (also called “folders”), which hold files or other directories.\nSeveral commands are frequently used to create, inspect, rename, and delete files and directories. To start exploring them, let’s open a shell window:\njae@jae-X705UDR:~$ \nLet’s demystify the output above. There’s nothing complicated.\n\njae: a specific user name\njae-X705UDR: your computer/server name\n~: current directory (~ = home)\n$: a prompt, which shows us that the shell is waiting for input; your shell may show something more elaborate.\n\nType the command whoami, then press the Enter key (sometimes marked Return) to send the command to the shell.\nThe command’s output is the ID of the current user, i.e., it shows us who the shell thinks we are:\n$ whoami\n\n# Should be your user name \njae \nMore specifically, when we type whoami the shell, the following sequence of events occurs behind the screen.\n\nFinds a program called whoami,\nRuns that program,\nDisplays that program’s output, then\nDisplays a new prompt to tell us that it’s ready for more commands.\n\n\n\nCommunicating to other systems\nIn the next unit, we’ll focus on the structure of our own operating systems. But our operating systems rarely work in isolation; we often rely on the Internet to communicate with others! You can visualize this sort of communication within your own shell by asking your computer to ping (based on the old term for submarine sonar) an IP address provided by Google (8.8.8.8); in effect, this will test whether your Internet is working.\n$ ping 8.8.8.8\nNote: Windows users may have to try a slightly different alternative:\n$ ping -t 8.8.8.8\n(Thanks Paul Thissen for the suggestion!). Note: press ctrl + C to stop your terminal pinging!\n\n\nFile system organization\nNext, let’s find out where we are by running a pwd command (print working directory).\nAt any moment, our current working directory is our current default directory, i.e., the directory that the computer assumes we want to run commands in unless we explicitly specify something else.\nHere, the computer’s response is /home/jae, which is the home directory:\n$ pwd\n\n/home/jae\n\nHome Directory\nThe home directory path will look different on different operating systems. For example, on Linux, it will look like /home/jae, and on Windows, it will be similar to C:\\Documents and Settings\\jae. Note that it may look slightly different for different versions of Windows.\n\n\nwhoami\nIf the command to find out who we are is whoami, the command to find out where we are ought to be called whereami, so why is it pwd instead? The usual answer is that in the early 1970s, when Unix was first being developed, every keystroke counted: the devices of the day were slow, and backspacing on a teletype was so painful that cutting the number of keystrokes to cut the number of typing mistakes was a win for usability. The reality is that commands were added to Unix one by one, without any master plan, by people who were immersed in its jargon.\nThe good news: because these basic commands were so integral to the development of early Unix, they have stuck around and appear (in some form) in almost all programming languages.\n\n\nIf you’re working on a Mac, the file structure will look similar, but not identical. The following image shows a file system graph for the typical Mac.\n\n\n\n\nFile Directory\n\n\nWe know that our current working directory /home/jae is stored inside /home because /home is the first part of its name. Similarly, we know that /home is stored inside the root directory / because its name begins with /.\n\n\nListing\nLet’s see what’s in your home directory by running ls (**list files and directories):\n$ ls\n\nApplications        Dropbox         Pictures\nCreative Cloud Files    Google Drive        Public\nDesktop         Library         Untitled.ipynb\nDocuments       Movies          anaconda\nDownloads       Music           file.txt\nls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns.\nWe can make ls more useful by adding flags. For instance, you can make your computer show only directories in the file system using the following command. Here -F flag classifies files based on some types. For example, / indicates directories.\nls -F /\nThe leading / tells the computer to follow the path from the file system’s root, so it always refers to exactly one directory, no matter where we are when we run the command.\nIf you want to see only directories in the current working directory, you can do the following. (Remember ^? This wildcard identifies a single number of characters. In this case, `d’.)\nls -l | grep \"^d\"\nWhat if we want to change our current working directory? Before we do this, pwd shows us that we’re in /home/jae, and ls without any arguments shows us that directory’s contents:\n$ pwd\n\n/home/jae\n\n$ ls\n\nApplications        Dropbox         Pictures\nCreative Cloud Files    Google Drive        Public\nDesktop         Library         Untitled.ipynb\nDocuments       Movies          anaconda\nDownloads       Music           file.txt\nUse relative paths (e.g., ../spring_2021/references.md) whenever it’s possible so that your code is not dependable on how your system is configured.\nAdditional tips\nHow can I find pdf files in Downloads using the terminal? Remember * wildcard?\ncd Downloads/ \n\nfind *.pdf\nAlso, note that you don’t need to type every character. Type the first few characters, then press TAB (autocomplete). This is called tab-completion, and we will see it in R as we go on.\n\n\nMoving around\nWe can use cd (change directory) followed by a directory name to change our working directory. First we need to get the file path, which we can do on a Mac the following ways and on a Windows computer like this.\n$ cd Desktop\ncd doesn’t print anything, but if we run pwd after it, we can see that we are now in /home/jae/Desktop.\nIf we run ls without arguments now, it lists the contents of /home/jae/Desktop, because that’s where we now are:\n$ pwd\n\n/home/jae/Desktop\nWe now know how to go down the directory tree: how do we go up? We could use an absolute path:\n$ cd /home/jae/\nbut it’s almost always simpler to use cd .. to go up one level:\n$ pwd\n\n/home/jae/Desktop\n\n$ cd ..\n.. is a special directory name meaning “the directory containing this one,” or more succinctly, the parent of the current directory. Sure enough, if we run pwd after running cd .., we’re back in /home/jae/:\n$ pwd\n\n/home/jae/\nThe special directory .. doesn’t usually show up when we run ls. If we want to display it, we can give ls the `-a’ flag:\n$ ls -a\n\n.       .localized  Shared\n..      Guest       rachel\n-a' stands for \"show all\"; it forceslsto show us file and directory names that begin with., such as..`.\n\nHidden Files: For Your Own Protection\nAs you can see, many other items just appeared when we enter ls -a'. These files and directories begin with.` followed by a name. Usually, files and directories hold important programmatic information. They are kept hidden so that users don’t accidentally delete or edit them without knowing what they’re doing.\n\nAs you can see, it also displays another special directory that’s just called ., which means “the current working directory”. It may seem redundant to have a name for it, but we’ll see some uses for it soon.\nAdditional tips\nThe above navigating exercises help us know about cd command, but not very exciting. So let’s do something more concrete and potentially useful. Let’s say you downloaded a file using your web browser and locate that file. How could you do that?\nYour first step should be learning more about the ls command. You can do that by Googling or typing ls --help. By looking at the documentation, you can recognize that you need to add -t (sort by time). In other words, here we’re looking for the most recently created files in our working directory.\nThen, what’s |? It’s called pipe, and it chains commands. For instance, if <command 1> | <command 2>, then command1’s output will be command2’s input. head list the first ten lines of a file. -n1 flag makes it show only the first line of the output (n1).\n# Don't forget to use TAB completion\ncd Downloads/ \n\nls -t | head -n1\nYeah! We can do more cool things. For example, how can you find the most recently downloaded PDF file? You can do this by combining the two neat tricks you learned earlier.\nls -t | find *.pdf | head -n1 \n\n\nCreating, copying, removing, and renaming files\n\nCreating files\n\nFirst, let’s create an empty directory named exercise\n\n\nmkdir exercise \n\nYou can check whether the directory is created by typing ls. If the print format is challenging to read, add -l flag. Did you notice the difference?\nLet’s move to the exercise subdirectory and create a file named test\n\n\ncd exercise ; touch test ; ls \n\nRead test\n\n\ncat test \n\nHmn. It’s empty. Let’s add something there. > = overwrite\n\n\necho \"something\" > test ; cat test \n\nYeah! Can you add more? >> = append\n\n\necho \"anything\" >> test ; cat test \n\nRemoving “anything” from test is a little bit more complex because you need to know how to use grep (remember that we used this command in the very first example). Here, I just demonstrate that you can do this task using Bash, and let’s dig into this more when we talk about working with text files.\n\n\ngrep -v \"anything\" test\n\n\nCopying and Removing Files\n\nCan we make a copy of test? Yes!\n\n\ncp test test_1; cat \n\nCan we make 100 copies of test? Yes!\n\nYou can do this\n\ncp test test_1 \ncp test test_2\ncp test test_3 \n\n... \nor\n\nfor i in {1..100}; do cp test \"test_$i\"; done  \n\nCan you remove all of the test_ files?\n\nYou can do this\nrm test_1\nrm test_2\nrm test_3 \n\n...\nor\nrm test_*\nWhich one do you like?\n\nLet’s remove the directory.\n\n\ncd .. \n\nrm exercise/\nThe rm command should not work because exercise is not a file. Type rm --help and see which flag will be helpful. It might be `-d’ (remove empty directories).\nrm -d exercise/  \nOops. Still not working because the directory is not empty. Try this. Now, it works.\nrm -r exercise/ \nWhat’s -r? It stands for recursion (e.g., Recursion is a very powerful idea in programming and helps solve complex problems. We’ll come back to it many times (e.g., purrr::reduce() in R).\n\n\n\n What on Earth is Recursion? - Computerphile \n\n\nRenaming files\n\nUsing mv\n\nFirst, we will learn how to move files and see how it’s relevant for renaming files.\n\n# Create two directories \nmkdir exercise_1 ; mkdir exercise_2 \n\n# Check whether they were indeed created \nfind exer*\n\n# Create an empty file \ntouch exercise_1/test \n\n# Move to exercise_1 and check \ncd exercise_1 ; ls \n\n# Move this file to exercise_2 \nmv test ../exercise_2 \n\n# Move to exercise_2 and check \ncd exercise_2 ; ls \nWhat has mv got to do with renaming?\n\n[mv] [source] [destination]\n\n\nmv test new_test ; ls \n\nUsing rename\n\nmv is an excellent tool to rename one file. But how about renaming many files? (Note that your pwd is still exercise_2 where you have the new_test file.)\n\nfor i in {1..100}; do cp new_test \"test_$i.csv\"; done  \nThen install rename. Either sudo apt-get install -y rename or brew install rename (MacOS).\nBasic syntax: rename [flags] perlexpr (Perl Expression) files. Note that Perl is another programming language.\n# Rename every csv file to txt file \nrename 's/.csv/.txt/' *.csv\n\n# Check \nls -l\nThe key part is s/.csv/.txt/ = s/FIND/REPLACE\nCan you perform the same task using GUI? Yes, you can, but it would be more time-consuming. Using the command line, you did this via just one-liner(!). Keith Brandnam wrote an excellent book titled UNIX and Perl to the Rescue! (Cambridge University Press 2012) that discusses how to use UNIX and Perl to deal with massively large datasets.\n\n\n\nWorking with CSV and text files\n\nDownload a CSV file (Forbes World’s Billionaires lists from 1996-2014). For more on the data source, see this site.\n\nwget https://corgis-edu.github.io/corgis/datasets/csv/billionaires/billionaires.csv\nNote you may first need to brew install wget (MacOS). To check whether it is installed on your machine, type:\nwget -V\nIf you’re using Windows and you don’t find wget, try following these steps.\n\nRead the first two lines. cat is printing, and head shows the first few rows. -n2 limits these number of rows equals 2.\n\nAdditional tips 1 If you have a large text file, cat prints everything at once is inconvenient. The alternative is using less.\ncat billionaires.csv | head -n2\nTo print the whole data in more legible format with column separation, we can type:\ncat billionaires.csv | column -t -s, | less -S\n\nCheck the size of the dataset (2615 rows). So, there are 2014 observations (n-1 because of the header). wc prints newline, word, and byte counts for each file. If you run wc without -l flag, you get the following: 2615 (line) 20433 (word) 607861 (byte) billionaires.csv\n\nwc -l billionaires.csv\n\nHow about the number of columns? sed is a stream editor and very powerful when it’s used to filter text in a pipeline. For more information, see this article. You’ve already seen s/FIND/REPLACE. Here, the pattern we are using is s/delimiter/\\n/g. We’ve seen that the delimiter is , so that’s what I plugged in the command below.\n\nhead -1 billionaires.csv | sed 's/,/\\n/g' | nl\nAdditional tips 2 The other cool command for text parsing is awk. This command is handy for filtering.\n\nThis is the same as using cat. So, what’s new?\n\nawk '{print}' billionaires.csv \n\nThis is new.\n\nawk '/China/ {print}' billionaires.csv\n\nLet’s see only the five rows. We filtered rows so that every row in the final dataset contains ‘China.’\n\nawk '/China/ {print}' billionaires.csv | head -n5 \n\nYou can also get the numbers of these rows.\n\nawk '/China/ {print NR}' billionaires.csv \n\n\nUser roles and file permissions\n\nIf you need admin access, use sudo. For instance, sudo apt-get install <package name> installs the package.\nTo run a Shell script (.sh), you need to change its file mode. You can make the script executable by typing chmod +x <Shell script>.\nThen, you can run it by typing ./pdf_copy_sh. The . here refers to the current working directory.\n\nNote: Other options to do the same thing: sh pdf_copy_sh. or bash pdf_copy_sh.\n\n\nWriting your first Shell script (.sh)\nFinally, we’re learning how to write a Shell script (a file that ends with .sh). Here I show how to write a Shell script that creates a subdirectory called /pdfs under /Download directory, then find PDF files in /Download and copy those files to pdfs. Essentially, this Shell script creates a backup. Name this Shell script as ‘pdf_copy.sh.’\n\n#!/bin/sh # Stating this is a Shell script. \n\nmkdir /home/jae/Downloads/pdfs # Obviously, in your case, this file path would be incorrect.\n\ncd Downloads\n\ncp *.pdf pdfs/ \n\necho \"Copied pdfs\"\nYou should now have a backup of all the PDFs that were in you Downloads folder!\nAdditional resources\n\n\n\n Using make and writing Makefile (in C++ or C) by Programming Knowledge \n\n\n\nReferences\n\nThe Unix Workbench by Sean Kross\nThe Unix Shell, Software Carpentry\nData Science at the Command Line by Jeroen Janssens\n\n\n\n\n Obtaining, Scrubbing, and Exploring Data at the Command Line by Jeroen Janssens from YPlan, Data Council \n\nShell Tools and Scripting, ./missing-semester, MIT\nCommand-line Environment, ./missing-semester, MIT"
  },
  {
    "objectID": "terminal_and_git.html#git-and-github",
    "href": "terminal_and_git.html#git-and-github",
    "title": "Managing data and code",
    "section": "Git and GitHub",
    "text": "Git and GitHub\n\nThe Big Picture\nThe most important point\n\nBackup != Version control\nIf you do version control, you need to save your raw data in your hard disk, external drive, or cloud, but nothing else. In other words, anything you are going to change should be subject to version control (also, it’s not the same as saving your code with names like 20200120_Kim or something like that). Below, I will explain what version control is and how to do it using Git and GitHub.\n\n\n\n\nWhy you should do version control\n\n\n\n\nVersion control system\nAccording to GitHub Guides, a version control system “tracks the history of changes as people and teams collaborate on projects together.” Specifically, it helps to track the following information:\n\nWhich changes were made?\nWho made the changes?\nWhen were the changes made?\nWhy were changes needed?\n\nGit is a case of a distributed version control system, common in open source and commercial software development. This is no surprise given that Git was originally created to deal with Linux kernel development.\nThe following images, from Pro Git, show how a centralized (e.g., CVS, Subversion, and Perforce) and decentralized VCS (e.g., Git, Mercurial, Bazzar or Darcs) works differently.\n\n\n\nCentralized version control system\n\n\nFigure 2. Centralized VCS.\n\n\n\nDecentralized version control system\n\n\nFigure 3. Decentralized VCS.\nFor more information on the varieties of version control systems, please read Petr Baudis’s review on that subject.\n\n\n\n Webcast • Introduction to Git and GitHub • Featuring Mehan Jayasuriya, GitHub Training & Guides \n\n\n\nFigure 2.1. A schematic git workflow from Healy’s “The Plain Person’s Guide to Plain Text Social Science”\n\n\nFor more information, watch the following video:\n\n\n\n The Basics of Git and GitHub, GitHub Training & Guides \n\n\nSetup\n\nSignup\n\nMake sure you have installed Git ([tutorial]).\n\ngit --version \n# git version 2.xx.x\n\nIf you haven’t, please sign up for a GitHub account: https://github.com/\n\n\nIf you’re a student, please also sign up for GitHub Student Developer Pack: https://education.github.com/pack Basically, you can get a GitHub pro account for free (so why not?).\n\n\nAccess GitHub using Hypertext Transfer Protocol Secure (HTTPS) or Secure Shell (SSH).\n\nHTTPS\n\nCreate a personal access token. Follow this guideline: https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token\nStore your credential somewhere safe. You can use an R package like this gitcreds and credentials to do so.\n\n\ninstall.packages(\"gitcreds\")\nlibrary(gitcreds)\n\n# First time only \ngitcreds_set()\n\n# Check \ngitcreds_get()\n\n\nIf you get asked to provide your password when you pull or push, the password should be your GitHub token (to be precise, personal access token).\n\nSSH\nIf possible, I highly recommend using SSH. Using SSH is safer and also makes connecting GitHub easier. SSH has two keys (public and private). The public key could be stored on any server (e.g., GitHub) and the private key could be saved in your client (e.g., your laptop). Only when the two are matched, the system unlocks.\n\nFirst, read this tutorial and create SSH keys.\nSecond, read this tutorial and check the keys and provide the public key to GitHub and add the private key to ssh-agent.\n\nNext time, if you want to use SSH, remember the following.\n# SSH\ngit@github.com:<user>/<repo>.git\n\n# HTTPS\nhttps://github.com/<user>/<repo>.git\n\n\nConfigurations\n\nMethod 1: using the terminal\n\n# User name and email \n$ git config --global user.name \"Firstname Lastname\"\n$ git config --global user.email username@school.extension\nThe first of this config calls asks for your user name as specified on Github.\n\nMethod 2: using RStudio\n\n\ninstall.packages(\"usethis\")\nlibrary(usethis)\n\nuse_git_config(user.name = \"<Firstname Lastname>\",\n               user.email = \"<username@school.extension>\")\n\nYou’re all set!\n\n\n\nCloning a repository\nLet’s clone a repository. Here, we’re actually going to clone the repository for my course book.\ngit clone https://github.com/cjbarrie/CS-ED\nIf you cd CS-ED/ you can move to the cloned course repository. Cloning: copying a public GitHub repo (remote) -> Your machine\nIf you accidentally changed something up in your local copy, you can just overwrite the local copy using the remote repo and make it exactly looks like the latter.\n# Download content from a remote repo \ngit fetch origin\n\n# Going back to origin/main\ngit reset --hard origin/main \n\n# Remove local files \ngit clean -f\nAdditional tips You can see cloning and forking on GitHub, and they sound similar. Let me differentiate them.\n\nCloning: creating a local copy of a public GitHub repo. In this case, you have writing access to the repo.\nForking (for open source projects): creating a copy of a public GitHub repo to your GitHub account, then you can clone it. In this case, you don’t have writing access to the repo. You need to create pull requests if you want your changes reflected in the original repo. Don’t worry about pull requests, as I will explain the concept shortly. For more information, see this documentation.\n\n\n\nMaking a repository\nCreate a new directory and move there. Then initialize\n# new directory \n$ mkdir code_exercise\n# move \n$ cd code_exercise \n# initialize\n$ git init \nAlternatively, you can create a Git repository via GitHub and then clone it on your local machine. Perhaps, it is an easier path for new users (I also do this all the time).\nWe can do this in the following way. First, go to you Github profile and click on the Repositories tab. Then click on “New.” You’ll be asked to give the Repo a name and an optional description. I highly recommend adding README (more on why we do this in the following subsection). We will discuss later on what a .gitignore file is for.\n\nOnce you have created your Repo you can then get the files on your local machine by calling the following:\n$ git clone /path/to/repository\nWhere the path will just be the URL for your Repository.\nAdditional tips If you’re unfamiliar with basic Git commands, please refer to this Git cheat sheet.\n\n\nCommit changes\nThese features show how Git works as a version control system.\nIf you edited files or added new ones, you need to update your repository. In Git terms, this action is called committing changes.\nMy current pwd is CS-ED. I created a text file named test containing text chris. You can check the file exists by typing `find “test```.\nThe following is a typical workflow to reflect this change to the remote.\n$ git status # check what's changed. \n$ git add . # update every change. In Git terms, you're staging. \n$ git add file_name # or stage a specific file.\n$ git commit -m \"your comment\" # your comment for the commit. \n$ git push origin main # commit the change. Origin is a default name given to a server by Git. `origin main` are optional. \nAnother image from Pro Git nicely illustrates this process.\n\n\n\nGit Workflow\n\n\nIf you made a mistake, don’t panic. You can’t revert the process.\ngit reset --soft HEAD~1 # if you still want to keep the change, but you go back to t-1 \ngit reset --hard HEAD~1 # if you're sure the change is unnecessary \nWriting an informative commit is essential. To learn how to do this better, see the following video:\n\n\n\n Your Commits Should Tell a Story • Featuring Eliza Brock Marcum, GitHub Training & Guides \n\n\nPush and pull (or fetch)\nThese features show how Git works as a collaboration tool.\nIf you have not already done it, let’s clone the PS239T directory on your local machine.\n$ git clone https://github.com/PS239T/spring_2021 # clone \nAdditional tips 1\nIf you try to remove spring_2021 using rm -r spring_2021/, you will get an error about the write-protected regular file. Then, try rm -rf spring_2021/.\nThen, let’s learn more about the repository.\n$ git remote -v \nYou should see something like the following:\norigin  git@github.com:PS239T/spring_2021 (fetch)\norigin  git@github.com:PS239T/spring_2021 (push)\nIf you want to see more information, then type git remote show origin.\nPreviously, we learned how to send your data to save in the local machine to the remote (the GitHub server). You can do that by editing or creating files, committing, and typing git push.\nInstead, if you want to update your local data with the remote data, you can type git pull origin (something like pwd in bash). Alternatively, you can use fetch (retrieve data from a remote). Git retrieves the data and merges it into your local data when you do that.\n$ git fetch origin\nAdditional tips 2\nDevelopers usually use PR to refer pull requests. When you are making PRs, it’s recommended to scope down (small PRs) because they are easier on reviewers and to test. To learn about how to accomplish this, see this blog post by Sarah Drasner.\n\n\nBranching\nIt’s an advanced feature of Git’s version control system that allows developers to “diverge from the main line of development and continue to do work without messing with that main line,” according to Scott Chacon and Ben Straub.\nIf you start working on a new feature, create a new branch.\n$ git branch new_features\n$ git checkout new_features\nYou can see the newly created branch by typing git branch.\nIn short, branching makes Git works like a mini file system.\n\n\nOther useful commands\n\nFor tracking history\n\n$ git diff # to see what changed (e.g., inside a file)\n$ git log # to track who committed what\n$ git log -S <pattern> # you can find a log that contains the pattern \n$ git checkout # to recover old files \n$ git revert # revert to the previous commit \n\nFor removing and renaming files\n\n$ git rm file_name # remove \n$ git mv old_file_name new_file_name # rename a file \nHow about removing a directory only from GitHub but not local?\ngit rm -r --cached <directory>\ngit commit -m \"<message>\"\ngit push\n\n\nCollaborations\nTwo options.\n\nSharing a repository (suitable for a private project).\nFork and pull (suitable for an open-source project). ​ * The one who maintains the repository becomes the maintainer. ​ * The others can fork, make changes, and even pull them back.\n\n\n\nDeployment: GitHub Pages\nUseful to deploy websites. I used the GitHub page to deploy this book.\n\n\n\n Webcast • Get Started with GitHub Pages • Featuring Dani Traphagen, GitHub Training & Guides \n\n\nTracking progress: GitHub Issues\nUseful to collect and respond to questions and suggestions (e.g., bug reports and feature suggestions) on the projects on which you’re working.\n\n\n\n Webcast • GitHub Issues • A Quick Look, GitHub Training & Guides \n\n\nProject management: GitHub Dashboards\nI use GitHub dashboards for almost every project that I have done.\n\n\n\n GitHub Projects Demo: Automation, Kanban, Scrum, Issues, Pull Request, Milestones, Issues, Tasks by Brandan Jones \n\n\nThe Big Picture\nWhen you are reading this section, please note that you’ve already grasped some key concepts behind R programming language (functions and objects).\nUNIX Commands (cat) = R Functions (print) Files = R Objects\n\n\nMotivation\nWhy do you need to make your research computationally reproducible?: for your own sanity and for public benefit. That is, it makes your life a whole lot easier to have a history of what you’ve done and why. And it makes research more transparent and reproducible: others can see what you’ve done, how you’ve done it—and they can go off and build on this.\n\nHow to organize files in a project\nYou won’t be able to reproduce your project unless it is efficiently organized.\nStep 1. Environment is part of your project. If someone can’t reproduce your environment, they won’t be able to run your code.\n\nLaunch R Studio. Tools > Global Options. You should not check Restore .RData into workspace at startup. Also, set the saving workspace option to NEVER.\n\nStep 2. For each project, create a project directory named after the project.\nname_of_the_project\n\n01_script_that_does_first_thing.R\n02_script_that_does_second_thing.R\ndata:\n\nraw\nprocessed (all processed, cleaned, and tided)\n\nfigures\nfunctions\nreports (PDF, HTML, TEX, etc.,)\nresults (model outcomes, etc.,)\n.gitignore (for Git)\nname_of_project.Rproj (for R)\nREADME.md (for Git)\n\n\n# Don't name it a project. Instead, use a more informative name. For instance, `us_election`, not `my_project.`\n\ndir.create(\"../us_election\")\n\nStep 3. Launch R Studio. Choose File > New project > Browse existing directories > Create project. This means each project has its own workspace.\nStep 4. Organize files by putting them in separate subdirectories and sensibly naming them.\n\nTreat raw data as read-only (raw data should be RAW!) and put it in the data subdirectory.\n\nAgain, note that version control does not need to replace backup. You still need to back up your raw data.\n\n\n\ndir.create(here::here(\"us_election\", \"data\"))\n\nAnd separate into raw and processed\n\ndir.create(here::here(\"us_election/data/\", \"raw\"))\ndir.create(here::here(\"us_election/data/\", \"processed\"))\n\n\nSeparate figures into the figures subdirectory.\n\n\ndir.create(here::here(\"us_election\", \"figures\"))\n\n\nPut any reports in the reports subdirectory.\n\n\ndir.create(here::here(\"us_election\", \"reports\"))\n\n\nPut generated results in the `results`` subdirectory and treat them as disposable.\n\n\ndir.create(here::here(\"us_election\", \"results\"))\n\n\nPut your custom functions in the functions subdirectory.\n\n\ndir.create(here::here(\"us_election\", \"functions\"))\n\nAre you tired of creating these directories one by one? Why not automate? See the following example. You can save this function as a rscript (e.g., setup.r) and run it in the terminal using Rscript <script name>.\n\nif (!require(pacman)) install.packages(\"pacman\")\n\n# Load here\npacman::p_load(\n  purrr, # functional programming\n  here # computational reproducibility\n)\n\n# Custom function\ncreate_dirs <- function(name) {\n  dir.create(here(name))\n}\n\n# Apply function \npurrr::map(c(\"data\", \"figures\", \"reports\", \"results\", \"functions\"), create_dirs)\n\nOf course, you don’t have to use these exact names. But here’s an example of how I tend to structure my directories—and you’ll see it follows this basic structure.\n\n\n\n\nHow to organize code in an R markdown file\n\nIn addition to environment, workflow is an essential component of project efficiency and reproducibility.\nWhat is R markdown? An R package, developed by Yihui Xie, provides an authoring framework for data science. Xie is also a developer of many widely popular R packages such as knitr, xaringan (cool kids use xaringan not Beamer these days), blogdown (used to create my personal website), and bookdown (used to create this book) among many others.\n\nMany applications: reports, presentations, dashboards, websites\n\nCheck out Communicating with R markdown workshop by Alison Hill (RStudio)\n\nAlison Hill is a co-author of blogdown: Creating Websites with R Markdown.\n\nKey strengths: dynamic reporting + reproducible science + easy deployment\n\n\n\n\n\nConcept map for R Markdown. By Gabriela Sandoval, Florencia D’Andrea, Yanina Bellini Saibene, Monica Alonso.\n\n\n\n\nR Markdown The bigger picture - Garrett Grolemund\n\n\nR-Ladies Oslo (English) - Reports to impress your boss! Rmarkdown magic - Athanasia Mowinckel\n\nR Markdown basic syntax\n\n\n# Header 1\n## Header 2\n### Header 3\n\n\nUse these section headers to indicate workflow.\n\n\n# Import packages and data\n# Tidy data\n# Wrangle data\n# Model data\n# Visualize data\n\nPress ctrl + shift + o. You can see a document outline based on these headers. This is a nice feature for finding the code you need to focus on.\nIf your project’s scale is large, divide these sections into files and numbers and save them in the code subdirectory.\n\n01_wrangling.Rmd\n02_modeling.Rmd …\n\n\nMaking a project computationally reproducible\n\nsetwd(): set a working directory.\nNote that using setwd() is not a reproducible way to set up your project. For instance, none will be able to run the following code except me.\n\n\n# Set a working directory \nsetwd(\"/home/jae/starwars\")\n\n# Do something \nggplot(mtcars, aes(x = mpg, y = wt)) +\n   geom_point()\n\n# Export the object. \n# dot means the working directory set by setwd()\nggsave(\"./outputs/example.png\") # This is called relative path \n\n\nInstead, learn how to use here()’.\n\nKey idea: separate workflow (e.g., workspace information) from products (code and data). For more information, please read Jenny Bryan’s excellent piece on project-oriented workflow.\nExample\n\n\n\n# New: Reproducible \n\nggplot(mtcars, aes(x = mpg, y = wt)) +\n   geom_point()\n\nggsave(here(\"project\", \"outputs\", \"example.png\"))\n\n\nHow here works\n\nhere() function shows what’s the top-level project directory.\n\nhere::here()\n\n\nBuild a path including subdirectories\n\n\nhere::here(\"project\", \"outputs\")\n           #depth 1   #depth 2\n\n\nHow here defines the top-level project directory. For example, the following list came from the here package vignette).\n\nIs a file named .here present?\nIs this an RStudio Project? (Note that we already set up an RStudio Project! So, if you use RStudio’s project feature, then you are ready to use here.)\nIs this an R package? Does it have a DESCRIPTION file?\nIs this a remake project? Does it have a file named remake.yml?\nIs this a projectile project? Does it have a file named .projectile?\nIs this a checkout from a version control system? Does it have a directory named .git or .svn? Currently, only Git and Subversion are supported.\nIf there’s no match then use set_here() to create an empty .here file.\n\n\n\n\n\nReferences\n\nCode and data management\n\n“Code and Data for the Social Sciences: A Practitioner’s Guide” by Matthew Gentkow and Jesse M. Shapiro\n\nProject-oriented research\n\nComputational reproducibility\n\n“Good Enough Practices in Scientific Computing” by PLOS\nProject Management with RStudio by Software Carpentry\nInitial steps toward reproducible research by Karl Broman\n\nVersion control\n\nVersion Control with Git by Software Carpentry\nThe Plain Person’s Guide to Plain Text Social Science by Kieran Healy"
  },
  {
    "objectID": "intror.html",
    "href": "intror.html",
    "title": "Introduction to R",
    "section": "",
    "text": "This section is designed to ensure you are familiar with the R environment."
  },
  {
    "objectID": "intror.html#getting-started-with-r-at-home",
    "href": "intror.html#getting-started-with-r-at-home",
    "title": "Introduction to R",
    "section": "Getting started with R at home",
    "text": "Getting started with R at home\nGiven that we’re all working from home these days, you’ll need to download R and RStudio onto your own devices. R is the name of the programming language that we’ll be using for coding exercises; RStudio is the IDE (“Integrated Development Environment”), i.e., the piece of software that almost everyone uses when working in R.\nYou can download both of these on Windows and Mac easily and for free. This is one of the first reasons to use an “open-source” programming language: it’s free and everyone can contribute!\nIT Services at the University of Edinburgh have provided a walkthrough of what is needed for you to get started. I also break this down below:\n\nInstall R for Mac from here: https://cran.r-project.org/bin/macosx/. Install R for Windows from here: https://cran.r-project.org/bin/windows/base/.\nDownload RStudio for Windows or Mac from here: https://rstudio.com/products/rstudio/download/, choosing the Free version: this is what most people use and is more than enough for all of our needs.\n\nAll programs are free. Make sure to load everything listed above for your operating system or R will not work properly!"
  },
  {
    "objectID": "intror.html#some-basic-information",
    "href": "intror.html#some-basic-information",
    "title": "Introduction to R",
    "section": "Some basic information",
    "text": "Some basic information\n\nA script is a text file in which you write your commands (code) and comments.\nIf you put the # character in front of a line of text this line will not be executed; this is useful to add comments to your script!\nR is case sensitive, so be careful when typing.\nTo send code from the script to the console, highlight the relevant line of code in your script and click on Run, or select the line and hit ctrl+enter on PCR or cmd+enter on Mac\nAccess help files for R functions by preceding the name of the function with ? (e.g., ?table)\nBy pressing the up key, you can go back to the commands you have used before\nPress the tab key to auto-complete variable names and commands"
  },
  {
    "objectID": "intror.html#getting-started-in-rstudio",
    "href": "intror.html#getting-started-in-rstudio",
    "title": "Introduction to R",
    "section": "Getting Started in RStudio",
    "text": "Getting Started in RStudio\nBegin by opening RStudio (located on the desktop). Your first task is to create a new script (this is where we will write our commands). To do so, click:\n\nFile --> NewFile --> RScript\n\nYour screen should now have four panes:\n\nthe Script (top left)\nthe Console (bottom left)\nthe Environment/History (top right)\nFiles/Plots/Packages/Help/Viewer (bottom right)"
  },
  {
    "objectID": "intror.html#a-simple-example",
    "href": "intror.html#a-simple-example",
    "title": "Introduction to R",
    "section": "A simple example",
    "text": "A simple example\nThe Script (top left) is where we write our commands for R. You can try this out for a first time by writing a small snipped of code as follows:\n\nx <- \"I can't wait to learn Computational Text Analysis\" #Note the quotation marks!\n\nTo tell R to run the command, highlight the relevant row in your script and click the Run button (top right of the Script) - or hold down ctrl+enter on Windows or cmd+enter on Mac - to send the command to the Console (bottom left), where the actual evaluation and calculations are taking place. These shortcut keys will become very familiar to you very quickly!\nRunning the command above creates an object named ‘x’, that contains the words of your message.\nYou can now see ‘x’ in the Environment (top right). To view what is contained in x, type in the Console (bottom left):\n\nprint(x)\n\n[1] \"I can't wait to learn Computational Text Analysis\"\n\n# or alternatively you can just type:\n\nx\n\n[1] \"I can't wait to learn Computational Text Analysis\""
  },
  {
    "objectID": "intror.html#loading-packages",
    "href": "intror.html#loading-packages",
    "title": "Introduction to R",
    "section": "Loading packages",
    "text": "Loading packages\nThe ‘base’ version of R is very powerful but it will not be able to do everything on its own, at least not with ease. For more technical or specialized forms of analysis, we will need to load new packages.\nThis is when we will need to install a so-called ‘package’—a program that includes new tools (i.e., functions) to carry out specific tasks. You can think of them as ‘extensions’ enhancing R’s capacities.\nTo take one example, we might want to do something a little more exciting than print how excited we are about this course. Let’s make a map instead.\nThis might sound technical. But the beauty of the packaged extensions of R is that they contain functions to perform specialized types of analysis with ease.\nWe’ll first need to install one of these packages, which you can do as below:\n\ninstall.packages(\"tidyverse\")\n\nAfter the package is installed, we then need to load it into our environment by typing library(). Note that, here, you don’t need to wrap the name of the package in quotation marks. So this will do the trick:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nWhat now? Well, let’s see just how easy it is to visualize some data using ggplot which is a package that comes bundled into the larger tidyverse package.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\nIf we wanted to save where we’d got to with making our plots, we would want to save our scripts, and maybe the data we used as well, so that we could return to it at a later stage."
  },
  {
    "objectID": "intror.html#saving-your-objects-plots-and-scripts",
    "href": "intror.html#saving-your-objects-plots-and-scripts",
    "title": "Introduction to R",
    "section": "Saving your objects, plots and scripts",
    "text": "Saving your objects, plots and scripts\n\nSaving scripts: To save your script in RStudio (i.e. the top left panel), all you need to do is click File –> Save As (and choose a name for your script). Your script will be something like: myfilename.R.\nSaving plots: If you have made any plots you would like to save, click Export (in the plotting pane) and choose a relevant file extension (e.g. .png, .pdf, etc.) and size.\nTo save individual objects (for example x from above) from your environment, run the following command (choosing a suitable filename):\n\n\nsave(x,file=\"myobject.RData\")\nload(file=\"myobject.RData\")\n\n\nTo save all of your objects (i.e. everything in the top right panel) at once, run the following command (choosing a suitable filename):\n\n\nsave.image(file=\"myfilname.RData\")\n\n\nYour objects can be re-loaded into R during your next session by running:\n\n\nload(file=\"myfilename.RData\")\n\nThere are many other file formats you might use to save any output. We will encounter these as the course progresses."
  },
  {
    "objectID": "intror.html#knowing-where-r-saves-your-documents",
    "href": "intror.html#knowing-where-r-saves-your-documents",
    "title": "Introduction to R",
    "section": "Knowing where R saves your documents",
    "text": "Knowing where R saves your documents\nIf you are at home, when you open a new script make sure to check and set your working directory (i.e. the folder where the files you create will be saved). To check your working directory use the getwd() command (type it into the Console or write it in your script in the Source Editor):\n\ngetwd()\n\nTo set your working directory, run the following command, substituting the file directory of your choice. Remember that anything following the `#’ symbol is simply a clarifying comment and R will not process it.\n\n## Example for Mac \nsetwd(\"/Users/Documents/mydir/\") \n## Example for PC \nsetwd(\"c:/docs/mydir\")"
  },
  {
    "objectID": "intror.html#practicing-in-r",
    "href": "intror.html#practicing-in-r",
    "title": "Introduction to R",
    "section": "Practicing in R",
    "text": "Practicing in R\nThe best way to learn R is to use it. These workshops on text analysis will not be the place to become fully proficient in R. They will, however, be a chance to conduct some hands-on analysis with applied examples in a fast-expanding field. And the best way to learn is through doing. So give it a shot!\nFor some further practice in the R programming language, look no further than Wickham and Grolemund (2017) and, for tidy text analysis, Silge and Robinson (2017).\n\nThe free online book by Hadley Wickham “R for Data Science” is available here\nThe free online book by Julia Silge and David Robinson “Text Mining with R” is available here\nFor more practice with R, you may want to consult a set of interactive tutorials, available through the package “learnr.” Once you’ve installed this package, you can go through the tutorials yourselves by calling:\n\n\nlibrary(learnr)\n\navailable_tutorials() # this will tell you the names of the tutorials available\n\nrun_tutorial(name = \"ex-data-basics\", package = \"learnr\") #this will launch the interactive tutorial in a new Internet browser window"
  },
  {
    "objectID": "intror.html#one-final-note",
    "href": "intror.html#one-final-note",
    "title": "Introduction to R",
    "section": "One final note",
    "text": "One final note\nOnce you’ve dipped into the “R for Data Science” book you’ll hear a lot about the so-called tidyverse in R. This is essentially a set of packages that use an alternative, and more intuitive, way of interacting with data.\nThe main difference you’ll notice here is that, instead of having separate lines for each function we want to run, or wrapping functions inside functions, sets of functions are “piped” into each other using “pipe” functions, which look have the appearance: %>%.\nI will be using “tidy” syntax in the weekly exercises for these computational text analysis workshops. If anything is unclear, I can provide the equivalents in “base” R too. But a lot of the useful text analysis packages are now composed with ‘tidy’ syntax.\n\n\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. London: O’Reilly.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. London: O’Reilly Media."
  },
  {
    "objectID": "introgen.html",
    "href": "introgen.html",
    "title": "Code style guide",
    "section": "",
    "text": "The following introductory section is taken, in slightly adapted form, from Jae Yeon Kim’s “Computational Thinking for Social Scientists.” It is reproduced here for ease of access. To consult the full book, go to https://jaeyk.github.io/comp_thinking_social_science/."
  },
  {
    "objectID": "introgen.html#the-big-picture",
    "href": "introgen.html#the-big-picture",
    "title": "Code style guide",
    "section": "The Big Picture",
    "text": "The Big Picture\n\nWhat is code style?\n\n\nEvery major open-source project has its style guide: a set of conventions (sometimes arbitrary) about writing code for that project. It is much easier to understand a large codebase when all the code in it is in a consistent style. - Google Style Guides\n\n\n\n\n10 Tips For Clean Code - Michael Toppa\n\nHow to avoid smelly code?\n\nCheck out the code-smells Git repository by Jenny Bryan.\n\n\n\n\n Code smells and feels - Jenny Bryan\n\n \"Code smell\" is an evocative term for that vague feeling of unease we get when reading certain bits of code. It's not necessarily wrong, but neither is it obviously correct. We may be reluctant to work on such code because past experience suggests it's going to be fiddly and bug-prone. In contrast, there's another type of code that just feels good to read and work on. What's the difference? If we can be more precise about code smells and feels, we can be intentional about writing code that is easier and more pleasant to work on. I've been fortunate to spend the last couple of years embedded in a group of developers working on the tidyverse and r-lib packages. Based on this experience, I'll talk about specific code smells and deodorizing strategies for R. - Jenny Bryan"
  },
  {
    "objectID": "introgen.html#write-readable-code",
    "href": "introgen.html#write-readable-code",
    "title": "Code style guide",
    "section": "Write readable code",
    "text": "Write readable code\n\nNaming matters\n\nWhen naming files, remember the following three rules:\n\nMachine-readable (avoid spaces, punctuation, periods, and any other special characters except _ and -)\nHuman readable (should be meaningful. No text1, image1, etc.,)\nOrdering (e.g., 01, 02, 03, … )\n\n\n\n\n# Good\nfit_models.R\n\n# Bad\nfit models.R\n\n\nWhen naming objects:\n\nDon’t use special characters.\nDon’t capitalize.\n\n\n\n# Good \nday_one\n    \n# Bad \nDayOne\n\n\nWhen naming functions:\n\nDon’t use special characters.\nDon’t capitalize.\nUse verbs instead of nouns. (Functions do something!)\n\n\n\n# Good \nrun_rdd \n\n# Bad \nrdd\n\n\nSpacing\n\nSome people do spacing by pressing the Tab key, and others do it by pressing the Space key multiple times (and this is a serious subject).\n\n\n\n Tabs versus Spaces \n\n# Good\nx[, 1] \n\nmean(x, na.rm = TRUE) \n\n# Bad\n\nx[,1]\n\nmean (x, na.rm = TRUE)\n\n\nIndenting\n\nIndent at least 4 spaces. Note that some people, including none other than Roger Peng, indent 8 spaces. The below example shows how you can change the default indentation setting using the RStudio configuration.\n\n\n\nRoger Peng’s tweet\n\n\n\n# Good\nif (y < 0) {\n  message(\"y is negative\")\n}\n\n# Bad\nif (y < 0) {\nmessage(\"Y is negative\")}\n\n\nLong lines\n\n\n# Good\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\"\n)\n\n# Bad\ndo_something_very_complicated(\"that\", requires = many, arguments =\n                              \"some of which may be long\"\n                              )\n\n\nComments\n\nUse comments to explain your decisions.\nBut, show your code; Do not try to explain your code by comments.\nAlso, try to comment out rather than delete the code you experiment with.\n\n\n\n# Average sleep hours of Jae\njae %>%\n  # By week\n  group_by(week) %>%\n  # Mean sleep hours \n  summarise(week_sleep = mean(sleep, na.rm = TRUE))\n\n\nPipes (chaining commands)\n\n\n# Good\niris %>%\n  group_by(Species) %>%\n  summarize_if(is.numeric, mean) %>%\n  ungroup() %>%\n  gather(measure, value, -Species) %>%\n  arrange(value)\n\n# Bad\niris %>% group_by(Species) %>% summarize_all(mean) %>%\nungroup %>% gather(measure, value, -Species) %>%\narrange(value)\n\n\nAdditional tips\nUse lintr to check whether your code complies with a recommended style guideline (e.g., tidyverse) and styler package to format your code according to the style guideline.\n\n\n\n\nhow lintr works\n\n\n\nWrite reusable code\n\nPasting\n\n\nCopy-and-paste programming, sometimes referred to as just pasting, is the production of highly repetitive computer programming code, as produced by copy and paste operations. It is primarily a pejorative term; those who use the term are often implying a lack of programming competence. It may also be the result of technology limitations (e.g., an insufficiently expressive development environment) as subroutines or libraries would normally be used instead. However, there are occasions when copy-and-paste programming is considered acceptable or necessary, such as for boilerplate, loop unrolling (when not supported automatically by the compiler), or certain programming idioms, and it is supported by some source code editors in the form of snippets. - Wikipedia\n\n\nIt’s okay for pasting for the first attempt to solve a problem. But if you copy and paste three times (a.k.a. Rule of Three in programming), something’s wrong. You’re working too hard. You need to be lazy. What do I mean, and how can you do that?\nThe following exercise was inspired by Wickham’s example.\nLet’s imagine df is a survey dataset.\n\na, b, c, d = Survey questions\n-99: non-responses\nYour goal: replace -99 with NA\n\n\n\n# Data\nlibrary(tibble)\nset.seed(1234) # for reproducibility \n\ndf <- tibble(\"a\" = sample(c(-99, 1:3), size = 5 , replace= TRUE),\n             \"b\" = sample(c(-99, 1:3), size = 5 , replace= TRUE),\n             \"c\" = sample(c(-99, 1:3), size = 5 , replace= TRUE),\n             \"d\" = sample(c(-99, 1:3), size = 5 , replace= TRUE))\n\n\n# Copy and paste \ndf$a[df$a == -99] <- NA\ndf$b[df$b == -99] <- NA\ndf$c[df$c == -99] <- NA\ndf$d[df$d == -99] <- NA\n\ndf\n\n# A tibble: 5 × 4\n      a     b     c     d\n  <dbl> <dbl> <dbl> <dbl>\n1     3     3     3     1\n2     3     2     3     1\n3     1    NA     1     2\n4     1    NA     2     1\n5    NA     1     1     3\n\n\n\nUsing a function\n\nfunction: input + computation + output\nIf you write a function, you gain efficiency because you don’t need to copy and paste the computation part.\n\n\n\n# Create a custom function\nfix_missing <- function(x) { # INPUT\n  x[x == -99] <- NA # COMPUTATION\n  x # OUTPUT \n}\n\n# Apply the function to each column (vector)\n# This iterated part can and should be automated.\ndf$a <- fix_missing(df$a)\ndf$b <- fix_missing(df$b)\ndf$c <- fix_missing(df$c)\ndf$d <- fix_missing(df$d)\n\ndf\n\n\nAutomation\n\nMany options for automation in R: for loop, apply family, etc.\nHere’s a tidy solution that comes from the purrr package.\nThe power and joy of one-liner.\n\n\n\ndf <- purrr::map_df(df, fix_missing) # What is this magic? We will unpack the blackbox (`map_df()`) later.\n\ndf\n\n\nTakeaways\n\n\nYour code becomes more reusable when it would be easier to change, debug, and scale-up. Don’t repeat yourself and embrace the power of lazy programming.\n\n\nLazy, because only lazy programmers will want to write the kind of tools that might replace them in the end. Lazy, because only a lazy programmer will avoid writing monotonous, repetitive code—thus avoiding redundancy, the enemy of software maintenance and flexible refactoring. Mostly, the tools and processes that come out of this endeavor fired by laziness will speed up the production. - Philipp Lenssen\n\n\nOnly when your code becomes reusable, you would become efficient in your data work. Otherwise, you need to start from scratch or copy and paste, when you work on a new project.\n\n\nCode reuse aims to save time and resources and reduce redundancy by taking advantage of assets that have already been created in some form within the software product development process.[2] The key idea in reuse is that parts of a computer program written at one time can be or should be used in the construction of other programs written at a later time. - Wikipedia\n\n\n\nTest your code systematically\nI strongly recommend switching from adhoc testing to formal automated testing (i.e., unit testing).\n\nWhenever you are tempted to type something into a print statement or a debugger expression, write it as a test instead. — Martin Fowler the author of Refactoring\n\n\n\n\n R language tip: Test your code with testthat by InfoWorld /p>\n\nif (!require(testthat)) install.packages(\"testthat\")\n\nLoading required package: testthat\n\npacman::p_load(testthat)\n\ncontext(\"Variable check\")\n\ntest_that(\"Check whether instructor variable is setup correctly\", {\n  \n  instructors <- c(\"Jae\", \"Nick\")\n\n  expect_equal(class(instructors), \"character\")\n\n}\n)\n\nTest passed 🎉\n\n\nInspired by an example in Hadley Wickham’s R Journal paper (2011).\n\ncontext(\"Model check\")\n\ntest_that(\"Check whether the model is lm\", {\n  \n  model <- lm(mpg ~ wt, data = mtcars)\n  \n  # Passes\n  expect_that(model, is_a(\"lm\"))\n\n  # Fails\n  expect_that(model, is_a(\"glm\"))\n\n})"
  },
  {
    "objectID": "introgen.html#run-tests",
    "href": "introgen.html#run-tests",
    "title": "Code style guide",
    "section": "Run tests",
    "text": "Run tests\n\ntest_file(file.choose()) # file \n\ntest_dir() # directory\n\nauto_test() # the test code tested when you save the file"
  },
  {
    "objectID": "introgen.html#asking-questions-minimal-reproducible-example",
    "href": "introgen.html#asking-questions-minimal-reproducible-example",
    "title": "Code style guide",
    "section": "Asking questions: Minimal reproducible example",
    "text": "Asking questions: Minimal reproducible example\n\nChances are you’re going to use StackOverflow a lot to solve a pressing problem you face. However, others can’t understand/be interested in your problem unless you provide an example that they can understand with minimal effort. Such an example is called a minimal reproducible example (MRE).\nRead this StackOverFlow post to understand the concept and best practices.\nSimply put, an MRE consists of the following items:\n\nA minimal dataset\nThe minimal burnable code\nThe necessary information on package, R version, system (use sessionInfo())\nA seed for reproducibility (set.seed()), if you used a random process.\n\n\nIn practice, use the reprex package to create the code component of the MRE.\n\nif (!require(reprex)) install.packages(\"reprex\")\n\nCopy the following code and type reprex() in the console.\n\ngpa <- c(3, 4, 4, 3)\nmean(gpa)"
  },
  {
    "objectID": "introgen.html#references",
    "href": "introgen.html#references",
    "title": "Code style guide",
    "section": "References",
    "text": "References\n\nWriting code\n\nStyle guides\n\nR\n\nGoogle’s R style guide\nR code style guide by Hadley Wickham\nThe tidyverse style guide by Hadley Wickham\n\nPython\n\nGoogle Python Style Guide\nCode Style by the Hitchhiker’s Guide to Python"
  },
  {
    "objectID": "repro_steps.html",
    "href": "repro_steps.html",
    "title": "Reproducible workflows",
    "section": "",
    "text": "This is a walkthrough example of how we might go about producing a reproducible piece of research.\nThere are several steps to this:"
  },
  {
    "objectID": "repro_steps.html#setting-up-github-credentials",
    "href": "repro_steps.html#setting-up-github-credentials",
    "title": "Reproducible workflows",
    "section": "Setting up Github credentials",
    "text": "Setting up Github credentials\nOnce we’ve got our Github account, we’re ready to get our computer talking to Github. We do this in the following way:\n\ninstall.packages(\"usethis\") #if you haven't already installed\nlibrary(usethis)\n\nusethis::create_github_token()\n\ngitcreds::gitcreds_set()\n\nAnd you can do this directly in RStudio like this:\n\nYou log your credentials there and you’re all set."
  },
  {
    "objectID": "repro_steps.html#creating-a-github-repo",
    "href": "repro_steps.html#creating-a-github-repo",
    "title": "Reproducible workflows",
    "section": "Creating a Github Repo",
    "text": "Creating a Github Repo\nWe first need to use our Github account (that we made in step one) by creating a “Repo.” This is just the place where we’re going to be logging and storing all our code and materials.\nTo do this, we go on to our Github account a select “New” next to the “Recent Repositories” tab. Mine looks like this:"
  },
  {
    "objectID": "repro_steps.html#section",
    "href": "repro_steps.html#section",
    "title": "Reproducible workflows",
    "section": "",
    "text": "Once we’ve done that, we can name and describe our Repo as below. We also add a “README,” which will be a kind of landing page online but will serve as the main explainer file should anyone want to download and reproduce your research.\n\nWe then press Create and you’ll be redirected to your new repository:\n\nYou’re ready for the next step!"
  },
  {
    "objectID": "repro_steps.html#creating-an-r-project",
    "href": "repro_steps.html#creating-an-r-project",
    "title": "Reproducible workflows",
    "section": "Creating an R Project",
    "text": "Creating an R Project\nNext we create an R Project by selection File –> New Project.\nYou’ll see a window like this pop up:\n\nAnd you should select Version Control, whereupon you’ll be asked:\n\nto choose either Git or Subversion. Choose Git. Then you’ll be asked to put in the name of the URL of the Github Repo you’ve created.\n\nAnd here we will enter the URL of the Github Repo we just created: https://github.com/cjbarrie/CS-ED_example."
  },
  {
    "objectID": "repro_steps.html#we-start-writing-our-code",
    "href": "repro_steps.html#we-start-writing-our-code",
    "title": "Reproducible workflows",
    "section": "We start writing our code",
    "text": "We start writing our code\nNow that we’re in our new R Project, we can start adding some contents!\nAt the start, your directory will look something like this:\n\nWe add a new script by pressing the top left icon of some white paper with a green + sign next to it. Or we can go to File –> New File –> R Script.\nHere I’m just writing a dummy example where I write a pointless function that prints a string followed by “for Computational Sociology.”\n\nOnce we’ve done this we can “Commit” and “Push.” And remember when we Commit we describe what this “Commit” (basically a version at which you’ve placed a yard stick or marker) is doing. Here I write that I am adding the first script.\n\nThen commit and push to our Github Repo we’ve set up within this R Project.\nAt which point we can go back to our “Remote” version of our work (the version that’s hosted on Github) and see if the changes are recorded there:\n\nTa da! Congratulations: you’re on your to producing reproducible research :)."
  },
  {
    "objectID": "repro_steps.html#writing-up-in-markdown",
    "href": "repro_steps.html#writing-up-in-markdown",
    "title": "Reproducible workflows",
    "section": "Writing up in Markdown",
    "text": "Writing up in Markdown\nLet’s say we want to answer one of the questions in Worksheet 1.\nWe’d first open our R Project that we’ve already linked to Github by following the steps above.\nWithin this project, we’d then open a new .Rmd file as follows, selecting the pdf output format and naming it “Worksheet 1”.\n\nWe’ll be met with a document that looks something like this:\n\nThe section that reads\n\nknitr::opts_chunk$set(echo = TRUE)\n\nis just setting some default display options, e.g., the formatting of figure sizes and other optional typesetting parameters. We normally don’t need to worry about this—and these can also be set for each specific chunk of code (rather than as global defaults). You can read more about these here.\nWe can therefore remove all of the template content in order to add our own markdown:\n\nAnd we can now begin to add our answers by inserting our first code chunk. We can also describe what we did above it.\n\nNote that here we’re using eval = T and echo = T as we want our code to appear in the markdown output and to be computed (rather than just printed). When we set eval = T, we are telling our machines to run the code specified; when we specify echo = T, we are telling out machines to print the code in the pdf (or html or word) output.\nNow that we’re down, we can press “Render,” which will produce our pdf document.\nWe can now “Commit” and “Push” to Github to store this version of our work remotely.\n\n\nAfter which we should see it on our remote version as below:\n\nAnd we’re done! Now just rinse and repeat..."
  },
  {
    "objectID": "chap1.html",
    "href": "chap1.html",
    "title": "1  Week 1: Intro. to CSS",
    "section": "",
    "text": "This week will be dedicated to a more general introduction to computational social science and what it means to think “computationally.”"
  },
  {
    "objectID": "chap1.html#computational-data",
    "href": "chap1.html#computational-data",
    "title": "1  Week 1: Intro. to CSS",
    "section": "1.1 Computational data",
    "text": "1.1 Computational data\nIn essence, this means getting used to different types of data. These might be:\n\nSocial media data;\nImage data;\nSound data;\nVideo data;\nRemote sensing data;\nCall detail records\n\nThese data are “found” or “trace” data. They are not custom-made for social science; they can be repurposed, though, for answering questions in the social sciences."
  },
  {
    "objectID": "chap1.html#computational-thinking",
    "href": "chap1.html#computational-thinking",
    "title": "1  Week 1: Intro. to CSS",
    "section": "1.2 Computational thinking",
    "text": "1.2 Computational thinking\nWe’ll also be be talking about what it means to “think computationally.” In her article Jeanette Wing describes computational thinking as a “universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.” But what is computational thinking?\nIn summary, it asks:\n\nwhat is computable?;\nit states the difficulty of a problem in computational terms;\nit asks whether an approximate solution is good enough;\nand it requires imagination to abstract into an approximate solution;\nto problems that might apply across almost every academic discipline imaginable\n\nWe’ll discuss a concrete example of this in our lecture."
  },
  {
    "objectID": "chap1.html#essential-reading",
    "href": "chap1.html#essential-reading",
    "title": "1  Week 1: Intro. to CSS",
    "section": "1.3 Essential reading:",
    "text": "1.3 Essential reading:\n\nSalganik (2017)\nD. M. J. Lazer et al. (2020)\nD. Lazer et al. (2021)\nWing (2006)"
  },
  {
    "objectID": "chap1.html#slides",
    "href": "chap1.html#slides",
    "title": "1  Week 1: Intro. to CSS",
    "section": "1.4 Slides",
    "text": "1.4 Slides\nSlides for week one are available here\n\n\n\n\nLazer, David M. J., Alex Pentland, Duncan J. Watts, Sinan Aral, Susan Athey, Noshir Contractor, Deen Freelon, et al. 2020. “Computational Social Science: Obstacles and Opportunities.” Science 369 (6507): 1060–62. https://doi.org/10.1126/science.aaz8170.\n\n\nLazer, David, Eszter Hargittai, Deen Freelon, Sandra Gonzalez-Bailon, Kevin Munger, Katherine Ognyanova, and Jason Radford. 2021. “Meaningful Measures of Human Society in the Twenty-First Century.” Nature 595 (7866): 189–96. https://doi.org/10.1038/s41586-021-03660-7.\n\n\nSalganik, Matthew J. 2017. Bit by Bit: Social Research in the Digital Age. Princeton, NJ.: Princeton University Press.\n\n\nWing, Jeannette M. 2006. “Computational Thinking.” Communications of the ACM 49 (3): 33–35."
  },
  {
    "objectID": "chap2.html",
    "href": "chap2.html",
    "title": "2  Week 2: Filter bubbles and echo chambers",
    "section": "",
    "text": "This week will be focusing on a substantive problem relating to social media and politics. Namely, we will be thinking about so-called “echo chambers” or “filter bubbles.” These refer to platform produced—or algorithmically produced—information environments where the user is exposed only to one sort of political information.\nAnd, normally, this political content is composed of information and opinions agreeable to the user.\nWhy is this a problem? The argument runs that this is a problem because a healthy society requires citizens to hear, be exposed to, and debate opposing opinions. Without this, societies might become increasingly contentious and polarized.\nBut how do we study this? And what does computation have to do with it?"
  },
  {
    "objectID": "chap2.html#essential-reading",
    "href": "chap2.html#essential-reading",
    "title": "2  Week 2: Filter bubbles and echo chambers",
    "section": "2.1 Essential reading:",
    "text": "2.1 Essential reading:\n\nChen and Rohla (2018)\nGuess (2021)\nLevy (2021)"
  },
  {
    "objectID": "chap2.html#additional-reading",
    "href": "chap2.html#additional-reading",
    "title": "2  Week 2: Filter bubbles and echo chambers",
    "section": "2.2 Additional reading:",
    "text": "2.2 Additional reading:\n\nBail (2021) (chapters 1-3)\nFletcher, Robertson, and Nielsen (2021)"
  },
  {
    "objectID": "chap2.html#slides",
    "href": "chap2.html#slides",
    "title": "2  Week 2: Filter bubbles and echo chambers",
    "section": "2.3 Slides",
    "text": "2.3 Slides\nSlides for this week are available here\n\n\n\n\nBail, Chris. 2021. “Breaking the Social Media Prism.” In. Princeton University Press.\n\n\nChen, M. Keith, and Ryne Rohla. 2018. “The Effect of Partisanship and Political Advertising on Close Family Ties.” Science 360 (6392): 1020–24. https://doi.org/10.1126/science.aaq1433.\n\n\nFletcher, Richard, Craig T. Robertson, and Rasmus Kleis Nielsen. 2021. “How Many People Live in Politically Partisan Online News Echo Chambers in Different Countries?” Journal of Quantitative Description: Digital Media 1 (August). https://doi.org/10.51685/jqd.2021.020.\n\n\nGuess, Andrew M. 2021. “(Almost) Everything in Moderation: New Evidence on Americans’ Online Media Diets.” American Journal of Political Science 65 (4): 1007–22. https://doi.org/10.1111/ajps.12589.\n\n\nLevy, Ro’ee. 2021. “Social Media, News Consumption, and Polarization: Evidence from a Field Experiment.” American Economic Review 111 (3): 831870. https://doi.org/10.1257/aer.20191777."
  },
  {
    "objectID": "chap3.html",
    "href": "chap3.html",
    "title": "3  Week 3: Web tracking and networks",
    "section": "",
    "text": "Last week we discussed the question of online “echo chambers” or “filter bubbles.” For this, we encountered a number of articles employing new computational tools to study these dynamics.\nOne of these tools was web-tracking—a subject we will discuss in detail this week. We will walk through the steps for generating web-tracking data and also unpack what web-tracking data looks like. We will then be thinking about what problems these techniques solve and which they fail to answer.\nA new type of analysis, which we didn’t cover in the articles last week, is so-called “network analysis” where we look at the structure of information flow, human-to-human or user-to-user connections. Here, we will discuss the basic structure of network data as well as how it can be used to tell us something about selective exposure online.\nThe replication task for this week will give you an example of web tracking network data, what it looks like, and how we can manipulate it to speak to some of the questions in the echo chambers literature."
  },
  {
    "objectID": "chap3.html#essential-reading",
    "href": "chap3.html#essential-reading",
    "title": "3  Week 3: Web tracking and networks",
    "section": "3.1 Essential reading:",
    "text": "3.1 Essential reading:\n\nConover et al. (2011)\nSTIER et al. (2021)\nFlaxman, Goel, and Rao (2016)"
  },
  {
    "objectID": "chap3.html#additional-reading",
    "href": "chap3.html#additional-reading",
    "title": "3  Week 3: Web tracking and networks",
    "section": "3.2 Additional reading:",
    "text": "3.2 Additional reading:\n\nHalberstam and Knight (2016)\nBakshy, Messing, and Adamic (2015)\nMosleh et al. (2021)\nChen et al. (2021)"
  },
  {
    "objectID": "chap3.html#slides",
    "href": "chap3.html#slides",
    "title": "3  Week 3: Web tracking and networks",
    "section": "3.3 Slides",
    "text": "3.3 Slides\nSlides for this week are available here\n\n\n\n\nBakshy, Eytan, Solomon Messing, and Lada A. Adamic. 2015. “Exposure to Ideologically Diverse News and Opinion on Facebook.” Science 348 (6239): 1130–32. https://doi.org/10.1126/science.aaa1160.\n\n\nChen, Wen, Diogo Pacheco, Kai-Cheng Yang, and Filippo Menczer. 2021. “Neutral Bots Probe Political Bias on Social Media.” Nature Communications 12 (1). https://doi.org/10.1038/s41467-021-25738-6.\n\n\nConover, Michael, Jacob Ratkiewicz, Matthew Francisco, Bruno Goncalves, Filippo Menczer, and Alessandro Flammini. 2011. “Political Polarization on Twitter.” Proceedings of the International AAAI Conference on Web and Social Media 5 (1): 89–96. https://ojs.aaai.org/index.php/ICWSM/article/view/14126.\n\n\nFlaxman, Seth, Sharad Goel, and Justin M. Rao. 2016. “Filter Bubbles, Echo Chambers, and Online News Consumption.” Public Opinion Quarterly 80 (S1): 298–320. https://doi.org/10.1093/poq/nfw006.\n\n\nHalberstam, Yosh, and Brian Knight. 2016. “Homophily, Group Size, and the Diffusion of Political Information in Social Networks: Evidence from Twitter.” Journal of Public Economics 143 (November): 73–88. https://doi.org/10.1016/j.jpubeco.2016.08.011.\n\n\nMosleh, Mohsen, Cameron Martel, Dean Eckles, and David G. Rand. 2021. “Shared Partisanship Dramatically Increases Social Tie Formation in a Twitter Field Experiment.” Proceedings of the National Academy of Sciences 118 (7). https://doi.org/10.1073/pnas.2022761118.\n\n\nSTIER, SEBASTIAN, FRANK MANGOLD, MICHAEL SCHARKOW, and JOHANNES BREUER. 2021. “Post Post-Broadcast Democracy? News Exposure in the Age of Online Intermediaries.” American Political Science Review 116 (2): 768–74. https://doi.org/10.1017/s0003055421001222."
  },
  {
    "objectID": "chap4.html",
    "href": "chap4.html",
    "title": "4  Week 4: Misinformation and fake news",
    "section": "",
    "text": "For the last couple of weeks we have discussed questions of echo chambers and filter bubbles. For the next couple of weeks, we will be discussing another phenomenon that has seen lots of headlines of late: misinformation and fake news.\nThis is a phenomenon closely related to the rise of the Internet and so-called “high-choice” media environments where the production of information is less centralized than it was before.\nAnd fake news, as well as misinformation, have been linked to a set of important political outcomes—from the result of elections to conspiracy belief or vaccine hesitancy (in the Covid-19 age)."
  },
  {
    "objectID": "chap4.html#essential-reading",
    "href": "chap4.html#essential-reading",
    "title": "4  Week 4: Misinformation and fake news",
    "section": "4.1 Essential reading:",
    "text": "4.1 Essential reading:\nAllen et al. (2020)\nGuess, Nyhan, and Reifler (2020)\nVosoughi, Roy, and Aral (2018)"
  },
  {
    "objectID": "chap4.html#additional-reading",
    "href": "chap4.html#additional-reading",
    "title": "4  Week 4: Misinformation and fake news",
    "section": "4.2 Additional reading:",
    "text": "4.2 Additional reading:\nBail et al. (2019)\nGrinberg et al. (2019)\nJuul and Ugander (2021)"
  },
  {
    "objectID": "chap4.html#slides",
    "href": "chap4.html#slides",
    "title": "4  Week 4: Misinformation and fake news",
    "section": "4.3 Slides",
    "text": "4.3 Slides\nSlides for this week are available here\n\n\n\n\nAllen, Jennifer, Baird Howland, Markus Mobius, David Rothschild, and Duncan J. Watts. 2020. “Evaluating the Fake News Problem at the Scale of the Information Ecosystem.” Science Advances 6 (14): eaay3539. https://doi.org/10.1126/sciadv.aay3539.\n\n\nBail, Christopher A., Brian Guay, Emily Maloney, Aidan Combs, D. Sunshine Hillygus, Friedolin Merhout, Deen Freelon, and Alexander Volfovsky. 2019. “Assessing the Russian Internet Research Agency’s Impact on the Political Attitudes and Behaviors of American Twitter Users in Late 2017.” Proceedings of the National Academy of Sciences 117 (1): 243–50. https://doi.org/10.1073/pnas.1906420116.\n\n\nGrinberg, Nir, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson, and David Lazer. 2019. “Fake News on Twitter During the 2016 U.S. Presidential Election.” Science 363 (6425): 374–78. https://doi.org/10.1126/science.aau2706.\n\n\nGuess, Andrew M., Brendan Nyhan, and Jason Reifler. 2020. “Exposure to Untrustworthy Websites in the 2016 US Election.” Nature Human Behaviour 4 (5): 472–80. https://doi.org/10.1038/s41562-020-0833-x.\n\n\nJuul, Jonas L., and Johan Ugander. 2021. “Comparing Information Diffusion Mechanisms by Matching on Cascade Size.” Proceedings of the National Academy of Sciences 118 (46). https://doi.org/10.1073/pnas.2100786118.\n\n\nVosoughi, Soroush, Deb Roy, and Sinan Aral. 2018. “The Spread of True and False News Online.” Science 359 (6380): 1146–51. https://doi.org/10.1126/science.aap9559."
  },
  {
    "objectID": "chap5.html",
    "href": "chap5.html",
    "title": "5  Week 5: Linked surveys and social media data",
    "section": "",
    "text": "This week we’re going to be looking at two techniques for analyzing the digital world using computational techniques. And we’re going to be thinking about how we use these techniques to answer questions specific to the field of misinformation research.\nThe first are linked surveys. These are a type of survey that combine traditional survey analysis, often deployed through online means, with linked social media or other digital trace data. We’re going to be talking about challenges of probability and non-probability sampling online as well as questions of response and recall.\nThe second is a standalone example social media analysis. The analysis of social media data often combines multiple different tools in the computational researcher’s toolkit. In the worksheet for this week, we will have a chance to collect some of our social media data and analyze it."
  },
  {
    "objectID": "chap5.html#essential-reading",
    "href": "chap5.html#essential-reading",
    "title": "5  Week 5: Linked surveys and social media data",
    "section": "5.1 Essential reading:",
    "text": "5.1 Essential reading:\nGuess, Nagler, and Tucker (2019)\nGonzález-Bailón and De Domenico (2021)"
  },
  {
    "objectID": "chap5.html#additional-reading",
    "href": "chap5.html#additional-reading",
    "title": "5  Week 5: Linked surveys and social media data",
    "section": "5.2 Additional reading:",
    "text": "5.2 Additional reading:\nBarrie and Frey (2021)"
  },
  {
    "objectID": "chap5.html#slides",
    "href": "chap5.html#slides",
    "title": "5  Week 5: Linked surveys and social media data",
    "section": "5.3 Slides",
    "text": "5.3 Slides\nSlides for this week are available here\n\n\n\n\nBarrie, Christopher, and Arun Frey. 2021. “Faces in the Crowd: Twitter as Alternative to Protest Surveys.” Edited by Barbara Guidi. PLOS ONE 16 (11): e0259972. https://doi.org/10.1371/journal.pone.0259972.\n\n\nGonzález-Bailón, Sandra, and Manlio De Domenico. 2021. “Bots Are Less Central Than Verified Accounts During Contentious Political Events.” Proceedings of the National Academy of Sciences 118 (11). https://doi.org/10.1073/pnas.2013443118.\n\n\nGuess, Andrew, Jonathan Nagler, and Joshua Tucker. 2019. “Less Than You Think: Prevalence and Predictors of Fake News Dissemination on Facebook.” Science Advances 5 (1). https://doi.org/10.1126/sciadv.aau4586."
  },
  {
    "objectID": "chap6.html",
    "href": "chap6.html",
    "title": "6  Week 6: Polarization and Radicalization",
    "section": "",
    "text": "This week we’re going to be looking at two key political outcomes often attributed to the influence of new digital domains: polarization and radicalization.\nPolarization may refer to various outcomes. Most often, we’re referring to so-called “affective polarization;” i.e., dislike of opposing political groups.\nRadicalization, meanwhile, refers to the progressive adoption of extreme political beliefs and—potentially—forms of behaviour. One common refrain here is that algorithmic influence leads users to consume ever more radical content. These have been popularized in journalistic articles such as this one."
  },
  {
    "objectID": "chap6.html#essential-reading",
    "href": "chap6.html#essential-reading",
    "title": "6  Week 6: Polarization and Radicalization",
    "section": "6.1 Essential reading:",
    "text": "6.1 Essential reading:\nAllcott et al. (2020)\nLedwich and Zaitsev (2019)"
  },
  {
    "objectID": "chap6.html#additional-reading",
    "href": "chap6.html#additional-reading",
    "title": "6  Week 6: Polarization and Radicalization",
    "section": "6.2 Additional reading:",
    "text": "6.2 Additional reading:\nWaller and Anderson (2021)"
  },
  {
    "objectID": "chap6.html#slides",
    "href": "chap6.html#slides",
    "title": "6  Week 6: Polarization and Radicalization",
    "section": "6.3 Slides",
    "text": "6.3 Slides\n\n\n\n\nAllcott, Hunt, Luca Braghieri, Sarah Eichmeyer, and Matthew Gentzkow. 2020. “The Welfare Effects of Social Media.” American Economic Review 110 (3): 629676. https://doi.org/10.1257/aer.20190658.\n\n\nLedwich, Mark, and Anna Zaitsev. 2019. “Algorithmic Extremism: Examining YouTube’s Rabbit Hole of Radicalization.” https://doi.org/10.48550/ARXIV.1912.11211.\n\n\nWaller, Isaac, and Ashton Anderson. 2021. “Quantifying Social Organization and Political Polarization in Online Platforms.” Nature 600 (7888): 264–68. https://doi.org/10.1038/s41586-021-04167-x."
  },
  {
    "objectID": "webtracking_networks.html",
    "href": "webtracking_networks.html",
    "title": "Worksheet 1: Web tracking and networks",
    "section": "",
    "text": "This is a walk through example that explores how to use web-tracking data.\nWe’ll also be using some of the skills you’ve learned from R4DS so far, namely:\n\nCurating data with e.g. filter and select functions in the dplyr package\nVisualizing with the ggplot2 package\n\nLet’s first install and load these packages. The below code first checks if the package is installed. If it’s not installed it then installs it. By using the function library followed by the name of the package in parentheses, we load these packages into memory so we can use them:\n\n#if not already install, first run:\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nWe are going to use some data that I collected using the “Web Historian” Chrome Extension that you’ll be able to add to your own Chrome Browsers by following the steps here.\n\n\n\n\n\nBy clicking on the image above, you’ll be taken to the Web Historian homepage. Click through to the “install from the Chrome Store” link to add to your own browsers.\n\n\n\n\n\nAs long as you have Chrome installed, you’ll then be able to install this browser extension on your own computers.\nOnce you’ve spent some time using the extension, you’ll have a good amount of data on your browsing habits stored. We can then read in these data, which by default are output as a JSON file. We read this in using the jsonlite package. Remember to first install the package if you don’t already have it installed:\n\n#if not already installed, first run:\nif (!require(\"jsonlite\")) install.packages(\"jsonlite\")\nlibrary(jsonlite)\n\nwbdata <- jsonlite::fromJSON(\"data/web_historian_data.json\")\n\nIf you’re downloading these data locally onto your computers, you can use the following link to do so:\n\nwbdata <- jsonlite::fromJSON(gzcon(url(\"https://github.com/cjbarrie/CS-ED/blob/main/data/web_historian_data.json?raw=true\")))\n\nWhat do these data look like? We can take a look below:\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\n\n \n  \n    domain \n    searchTerms \n    urlId \n  \n \n\n  \n    google.com \n     \n    43 \n  \n  \n    google.com \n    jess philips politics uk twitter \n    1726 \n  \n  \n    google.com \n    jess philips politics uk twitter \n    1726 \n  \n  \n    google.com \n    chris bryant politics uk twitter \n    1727 \n  \n  \n    google.com \n    chris bryant politics uk twitter \n    1727 \n  \n\n\n\n\n\nThe data contain more columns than this, but we’ll focus on these three columns for now. How do we select just these three columns and thereby reduce the size of our dataset? To do so, we just need to use the select function in the dplyr package:\n\nwbdata_short <- wbdata %>% \n  select(domain, searchTerms, urlId)\n\nWhat’s the most popular website in my browsing behaviour? To find out, we’ll have to count how many times I’ve accessed a particular domain. This is pretty straightforward. We first group_by the domain of interest and then we use the count function to tally up the number of visits per domain:\n\nwbdata %>% \n  group_by(domain) %>%\n  count() \n\n# A tibble: 27 × 2\n# Groups:   domain [27]\n   domain                               n\n   <chr>                            <int>\n 1 Acrobat-for-Chrome.pdf Extension     2\n 2 Adobe Acrobat Options Extension      2\n 3 adobe.com                            3\n 4 amazon.co.uk                         6\n 5 apple.com                            6\n 6 atgtickets.com                       6\n 7 atypon.com                           2\n 8 bbc.co.uk                            2\n 9 chrome.com                           1\n10 chrome.google.com                    9\n# … with 17 more rows\n\n\nAt the moment, this is sorted alphabetically, which isn’t very intuitive. Let’s instead tell R to display in descending order the number of visits to each domain with the arrange function:\n\nwbdata %>% \n  group_by(domain) %>%\n  count() %>%\n  arrange(desc(n))\n\n# A tibble: 27 × 2\n# Groups:   domain [27]\n   domain                  n\n   <chr>               <int>\n 1 google.com             61\n 2 diversitytravel.com    24\n 3 qualtrics.com          11\n 4 chrome.google.com       9\n 5 stackoverflow.com       9\n 6 uchicago.edu            9\n 7 youtube.com             9\n 8 amazon.co.uk            6\n 9 apple.com               6\n10 atgtickets.com          6\n# … with 17 more rows\n\n\nWe can see clearly that I favour Google as a search engine… and that I’ve been doing some travel booking recently, as well as using the survey platform Qualtrics.\nWe can then gather together the most popular domains and save them in a separate data.frame object I’m calling wbdata_mp.\n\nwbdata_mp <- wbdata %>% \n  group_by(domain) %>%\n  count() %>%\n  filter(n >5)\n\nAnd once we have this, it’s pretty easy for us to start plotting the data like so:\n\nwbdata_mp %>%\n  ggplot() +\n  geom_bar(aes(domain, n), stat = \"identity\") +\n  coord_flip()\n\n\n\n\n\n\n\nCount the types of things I’m doing online (column “transType” in the wbdata). What is my most frequent type of interaction with the web?\nChange the colour of the bars in the bar graph."
  },
  {
    "objectID": "webtracking_networks.html#analyzing-network-data",
    "href": "webtracking_networks.html#analyzing-network-data",
    "title": "Worksheet 1: Web tracking and networks",
    "section": "Analyzing network data",
    "text": "Analyzing network data\n\n\n\n\n\n\nThis section is adapted in part from the exercise written by Tod Van Gunten for SICSS 22 here.\n\n\n\n\nlibrary(igraph)\nlibrary(tidygraph)\nlibrary(dplyr)\nlibrary(RColorBrewer)\nlibrary(classInt)\n\nconov_net <- read_graph(\"data/replication/icwsm_polarization/all.graphml\", format = \"graphml\")\n\nnodes <- read.table(\"data/replication/icwsm_polarization/all.nodes.txt\")\n\nedges <- read.table(\"data/replication/icwsm_polarization/all.edgelist.txt\")\n\nnodes %>% \n  head(5) %>%\n  kbl() %>%\n  kable_styling(c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    V1 \n    V2 \n  \n \n\n  \n    0 \n    right \n  \n  \n    1 \n    right \n  \n  \n    2 \n    left \n  \n  \n    3 \n    left \n  \n  \n    4 \n    - \n  \n\n\n\n\nedges %>% \n  head(5) %>%\n  kbl() %>%\n  kable_styling(c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    V1 \n    V2 \n    V3 \n    V4 \n    V5 \n  \n \n\n  \n    4522 \n    617 \n    retweet \n    1 \n    1285399493 \n  \n  \n    13126 \n    12049 \n    retweet \n    0 \n    1287197840 \n  \n  \n    11956 \n    13524 \n    retweet \n    0 \n    1288106297 \n  \n  \n    12875 \n    3983 \n    retweet \n    1 \n    1284533918 \n  \n  \n    10701 \n    13172 \n    reply \n    1 \n    1286226286 \n  \n\n\n\n\n\nHere we can see in the first table what we refer to in the networks world as “nodes” and in the second what we call “edges.”\nYou can download these data on your machines with:\n\nnodes <- read.table(url(\"https://github.com/cjbarrie/CS-ED/blob/main/data/all.nodes.txt?raw=true\"))\n\nedges <- read.table(url(\"https://github.com/cjbarrie/CS-ED/blob/main/data/all.edgelist.txt?raw=true\"))\n\nA node is the name for any individual entity in our networks (so here: a Twitter account) along with its “left-right” political leaning label.\nThe edges record the links between one account and another account. These might be “retweets” or they might be “mentions.” And we can visualize these separately.\nFor each of the nodes and edges, the numbers refer to the individual unique ID of the node and the unique ID of the edge. So, in the first row: 4522 and 617 refer to the IDs of a “tie” or link between one node and another node—and this is what we refer to as an “edge.”\n\nlibrary(tidylog)\n\ncolnames(nodes) <- c(\"node1\", \"lr\")\ncolnames(edges) <- c(\"node1\", \"node2\", \"type\", \"n\", \"id\")\nrt_samp <- edges %>%\n  left_join(nodes, by = \"node1\") %>%\n  filter(type == \"retweet\")\n\nNote here we’re loading a new package tidylog, which is helpful because it outputs information on the success or failure of any data joins we’re doing—and tells us what any filter has done to our data. It’s good practice to have this loaded when you’re wrangling data as it helps avoid mistakes.\nHere, we have first renamed our columns to something more intuitive: i.e., “node1” to refer to the nodes from which any tie is formed (because they retweet or mention another account) and “lr” to refer to the left-right orientation of the account that is doing the retweeting or mentioning.\nFor the edges data, we have done the same, but we also have columns for the node to which one node is making a connection by either retweeting or mentioning. Here, I’m referring to these accounts as “node2” because this is the node with which the edge is formed.\nWe then also have a column in the edges data for “type” and this refers to whether the edge between two nodes is a retweet or a mention. Finally, we have the “n” (or count) of retweets or mentions between two nodes, and a unique identifier or “id”.\n\nrt_samp$node1 <- as.character(rt_samp$node1)\nrt_samp$node2 <- as.character(rt_samp$node2)\n\nigraph_rt_samp <- graph_from_edgelist(\n  as.matrix(rt_samp[,c(\"node1\",\"node2\")]),\n  directed = T\n)\n\nplot(simplify(igraph_rt_samp), \n     vertex.label = NA, \n     vertex.size = 2,\n     edge.arrow.size = 0)\n\n\n\n\nIn the above, we have then converted the “class” of the $node1 and $node2 objects to the character class so we can properly reformat them into the matrix type class required to make an igraph object, which is the class of object we need in order to use the igraph package.\nOnce we have made our igraph object, which here we’re calling igraph_rt_samp since this is an igraph object containing a sample of retweets, we are ready to plot.\nHere, we are simply plotting the network, and specifying a few styling options, which change the size of the vertices (another name for nodes) and remove the default arrows that appear on these plots (as they just clutter the visualization).\n\nsamp_attr <- data.frame(\n  node = V(igraph_rt_samp)$name,\n  node.seq = 1:length(V(igraph_rt_samp)$name)\n  # ,\n  # degree.in = degree(igraph_rt_samp, mode = \"in\"), #unhash this to also estimate indegree\n  # between.dir = betweenness(igraph_rt_samp, directed = T,normalized = T), #unhash this to also estimate betweenness\n  # between.undir = betweenness(igraph_rt_samp, directed = F, normalized = T) #unhash this to also estimate betweenness (undirected)\n)\n\nnodes$node <- as.character(nodes$node)\nnodes <- nodes %>%\n  mutate(lrcolor = recode(lr,\n                              \"right\" = \"#DE0100\",\n                              \"left\" = \"#0015BC\",\n                              \"-\" = \"#a6a49f\")\n  )\n\nmutate: new variable 'lrcolor' (character) with 3 unique values and 0% NA\n\nsamp_attr_lr <- samp_attr %>%\n  left_join(nodes, by = \"node\")\n\nleft_join: added 3 columns (node1, lr, lrcolor)\n\n\n           > rows only in x        0\n\n\n           > rows only in y  ( 3,471)\n\n\n           > matched rows     18,934\n\n\n           >                 ========\n\n\n           > rows total       18,934\n\nplot(simplify(igraph_rt_samp), \n     vertex.label = NA, \n     vertex.size = 4,\n     vertex.color = samp_attr_lr$lrcolor,\n     edge.arrow.size = 0)\n\n\n\n\nNotice, however, that the above plot doesn’t display any information on the left-right orientation of the nodes (or accounts). To do this, we need to add in information on the ideological orientation of users. We can do this by using the mutate and recode functions in the tidyverse.\nFirst we create an object that just orders all our nodes into one data.frame object. Note you can unhash some of these rows to also get some additional statistics such as betweenness and indegree.\nWe choose hex colors based on the standard colors for the “Democrat” and “Republican” parties in the US: i.e., red for Republicans and blue for Democrats.\nWe then add this into our samp_attr_lr object where we have recorded the attributes of each of our edges.\nNow we can plot: and we see that we already have a network visualization that looks similar to that in Figure 1 of the Conover et al. (2011) article.\n\nQuestions\n\nCount how many “left” nodes there are and how many “right” nodes there are.\nAdd arrows to the network plot and increase their width (hint: see here)\n\n\n\n\n\nConover, Michael, Jacob Ratkiewicz, Matthew Francisco, Bruno Goncalves, Filippo Menczer, and Alessandro Flammini. 2011. “Political Polarization on Twitter.” Proceedings of the International AAAI Conference on Web and Social Media 5 (1): 89–96. https://ojs.aaai.org/index.php/ICWSM/article/view/14126."
  },
  {
    "objectID": "data_ingest_apis.html",
    "href": "data_ingest_apis.html",
    "title": "Worksheet 2: Importing data and APIs",
    "section": "",
    "text": "In what follows, we’ll go through some examples of how to collect data from Twitter. Twitter has a number of access points (APIs) and the amount and variety of data you can access depends on your permissions. These are explained below.\nOften, articles using Twitter data share the replication materials as tweet “IDs”. This is because the Terms of Service for the Twitter API dictate that, for reproduction purposes, it is not permitted to share the raw contents of tweets. Instead, we have to “rehydrate” these tweets. An example of this from Barrie and Frey (2021) is provided below."
  },
  {
    "objectID": "data_ingest_apis.html#using-apis",
    "href": "data_ingest_apis.html#using-apis",
    "title": "Worksheet 2: Importing data and APIs",
    "section": "Using APIs",
    "text": "Using APIs\nTo practice these skills, you might want to create a new Twitter account for your academic research. But you needn’t create a new account to follow the steps below. You can simply use your own Twitter account—if you have one—as using the developer tools will not change anything about your public Twitter account.\n\nBefore proceeding, we’ll load the remaining packages we will need for this tutorial.\n\nlibrary(tidyverse) # loads dplyr, ggplot2, and others\nlibrary(rtweet) # to query the Twitter API in R\n\nWarning: package 'rtweet' was built under R version 4.1.3\n\n\nOnce you’ve create your new account, or have logged into your existing one, go to the Twitter developer portal log in page here.\nClick on Apply in the navigation bar on the top right of the page. You’ll be asked “what best describes you?” For the purposes of this tutorial, select academic, and then select student. Fill in all the relevant information and submit your application. Your application will then be reviewed by Twitter before access is granted. This might take hours or days.\nOnce you have authorization, a new tab will appear in the navigation bar at the top of the develop portal, as below:\n\nNavigate to the developer portal and you will there be able to create a new “app” to query the API. You see in my account that I have several apps for different purposes.\n We can create a new app on this page too. When we click “Create App” we will first be asked to name the app. Most importantly, we will then be given an “API key”; an “API secret key”; and a “Bearer token” as below.\n\nYou MUST make a record of these. Once you have done so, you can then use these to access the API. Once you have recorded these, navigate to the App setting tabs for the App you’ve created now listed in the Overview tab on the left hand side navigation window.\n\nNavigate to “Keys and tokens” on this page, and click generate in the Access token & secret box as below:\n\nRecord these as well. Once you have all of these keys and tokens recorded somewhere safe, you are ready to collect data!\nThis is pretty simple using the rtweet package. Below, we’ll collect the last 50 tweets of the founder of Twitter: Jack Dorsey.\n\napi_key <-\" XXXXXXXXXXXXXXXXXXXXXXX\"\napi_key_secret <- \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\naccess_token <- \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\naccess_token_secret <- \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n\ntoken <- create_token(\n  app = \"NAME YOUR APP\",\n  consumer_key = api_key,\n  consumer_secret = api_key_secret,\n  access_token = access_token,\n  access_secret = access_token_secret\n)\n\nuser <- \"@jack\"\njacktweets <- get_timeline(user, n = 50)\n\nhead(jacktweets)\n\nOnce you have enter your keys, tokens, and key/token secrets, store them with the create_token() function. Here, we are collecting the last 50 tweets for Jack Dorsey, though you can change this by specifying a higher n—be aware, though, that the maximum you are able to collect with the basic API access is 3200 tweets.\n\n\n\n\n \n  \n    created_at \n    screen_name \n    text \n  \n \n\n  \n    2021-02-25 23:36:34 \n    jack \n    Bitcoin, Blockchain and the Black Community - CoinDesk https://t.co/3KH6W7Rg0Q \n  \n  \n    2021-02-25 23:32:42 \n    jack \n    Starting in less than an hour! 👇🏾\n\nTune into @CoinDesk TV and watch #CommunityCrypto at 5pm ET \n\nSee you then! ✊🏾 https://t.co/ACnRhdabEy \n  \n  \n    2021-02-25 04:22:10 \n    jack \n    Crypto art changing lives https://t.co/yDEQOaA483 \n  \n  \n    2021-02-25 03:59:05 \n    jack \n    https://t.co/2i46RZ6c0A \n  \n  \n    2021-02-24 04:47:59 \n    jack \n    @hfangca @OKCoin @opencryptoorg Thank you for joining! \n  \n  \n    2021-02-23 17:13:31 \n    jack \n    @bitcoinbrink 🙏🏼 \n  \n  \n    2021-02-20 02:06:24 \n    jack \n    It’s an honor to help in this important endeavor. All orgs developing in crypto — for-profit or nonprofit, even individuals — should consider joining. There is strength in numbers. https://t.co/CJo9frhkhJ \n  \n  \n    2021-02-20 01:47:23 \n    jack \n    Over 6,600 people submitted their proof-of-work so far. Working to narrow over weekend and will start interviewing board candidates soon. \n  \n  \n    2021-02-19 20:34:35 \n    jack \n    This is excellent 🧡 https://t.co/e7ywpoCcxS \n  \n  \n    2021-02-17 03:47:40 \n    jack \n    learned the most from https://t.co/GrUoCXanNL and https://t.co/cz3yYo4UEw \n  \n\n\n\n\n\nNow you can play around with the different API calls possible with the rtweet package. See the full documentation here and here.\nAnd for those interested, you can my package to collect tweets from the Academic Research Product Track API here.\nGetting access to the Academic Research Product Track is a bit more complicated but for more information on how to apply see here.\nIn order to use the Twitter Academic Research Product Track you will first need to obtain an authorization token. You will find details about the process of obtaining authorization here.\nIn order to gain authorization you first need a Twitter account.\nFirst, Twitter will ask for details about your academic profile. Per the documentation linked above, they will ask for the following:\n\nYour full name as it is appears on your institution’s documentation\nLinks to webpages that help establish your identity; provide one or more of the following:\n\nA link to your profile in your institution’s faculty or student directory\nA link to your Google Scholar profile\nA link to your research group, lab or departmental website where you are listed\n\nInformation about your academic institution: its name, country, state, and city\nYour department, school, or lab name\nYour academic field of study or discipline at this institution\nYour current role as an academic (whether you are a graduate student, doctoral candidate, post-doc, professor, research scientist, or other faculty member)\n\nTwitter will then ask for details of the proposed research project. Here, questions include:\n\n\nWhat is the name of your research project?\nDoes this project receive funding from outside your academic institution? If yes, please list all your sources of funding.\nIn English, describe your research project. Minimum 200 characters.\nIn English, describe how Twitter data via the Twitter API will be used in your research project. Minimum 200 characters.\nIn English, describe your methodology for analyzing Twitter data, Tweets, and/or Twitter users. Minimum 200 characters.\nWill your research present Twitter data individually or in aggregate?\nIn English, describe how you will share the outcomes of your research (include tools, data, and/or other resources you hope to build and share). Minimum 200 characters.\nWill your analysis make Twitter content or derived information available to a government entity?\n\n\nOnce you have gained authorization for your project you will be able to see the new project on your Twitter developer portal.\nThe Academic Research Product Track permits the user to access larger volumes of data, over a far longer time range, than was previously possible. From the Twitter documentation:\n\n“The Academic Research product track includes full-archive search, as well as increased access and other v2 endpoints and functionality designed to get more precise and complete data for analyzing the public conversation, at no cost for qualifying researchers. Since the Academic Research track includes specialized, greater levels of access, it is reserved solely for non-commercial use”.\n\nThe new “v2 endpoints” refer to the v2 API, introduced around the same time as the new Academic Research Product Track. Full details of the v2 endpoints are available here.\nIn summary the Academic Research product track allows the authorized user:\n\nAccess to the full archive of (as-yet-undeleted) tweets published on Twitter\nA higher monthly tweet cap (10m–or 20x what was previously possible with the standard v1.1 API)\nAbility to access these data with more precise filters permitted by the v2 API\n\nIf you do get authorization for using the Twitter Academic API, you can then follow the next steps to begin collecting data. Instead of four different keys or secrets, we will have one “bearer token” that we use, associated with one of the apps that we created as above. This is all we then need to begin collecting data.\nFirst we need to load the package into memory as follows.\n\nlibrary(academictwitteR)\n\nThe first task is set authorization credentials with the set_bearer() function, which allows the user to store their bearer token in the .Renviron file.\nTo do so, use:\n\nset_bearer()\n\nand enter authorization credentials as below:\n\nThis will mean that the bearer token is automatically called during API calls. It also avoids the inadvisable practice of hard-coding authorization credentials into scripts.\nThe workhorse function is get_all_tweets(), which is able to collect tweets matching a specific search query or all tweets by a specific set of users.\n\ntweets <-\n  get_all_tweets(\n    query = \"#BlackLivesMatter\",\n    start_tweets = \"2020-01-01T00:00:00Z\",\n    end_tweets = \"2020-01-05T00:00:00Z\",\n    file = \"blmtweets\",\n    data_path = \"data/\",\n    n = 1000000,\n  )\n\nHere, we are collecting tweets containing a hashtag related to the Black Lives Matter movement over the period January 1, 2020 to January 5, 2020.\nWe have also set an upper limit of one million tweets. When collecting large amounts of Twitter data we recommend including a data_path and setting bind_tweets = FALSE such that data is stored as JSON files and can be bound at a later stage upon completion of the API query.\n\ntweets <-\n  get_all_tweets(\n    users = c(\"jack\", \"cbarrie\"),\n    start_tweets = \"2020-01-01T00:00:00Z\",\n    end_tweets = \"2020-01-05T00:00:00Z\",\n    file = \"blmtweets\",\n    n = 1000\n  )\n\nWhereas here we are not specifying a search query and instead are requesting all tweets by users “jack” and “cbarrie” over the period January 1, 2020 to January 5, 2020. Here, we set an upper limit of 1000 tweets.\nThe search query and user query arguments can be combined in a single API call as so:\n\nget_all_tweets(\n  query = \"twitter\",\n  users = c(\"cbarrie\", \"jack\"),\n  start_tweets = \"2020-01-01T00:00:00Z\",\n  end_tweets = \"2020-05-01T00:00:00Z\",\n  n = 1000\n)\n\nWhere here we would be collecting tweets by users “jack” and “cbarrrie” over the period January 1, 2020 to January 5, 2020 containing the word “twitter.”\n\nget_all_tweets(\n  query = c(\"twitter\", \"social\"),\n  users = c(\"cbarrie\", \"jack\"),\n  start_tweets = \"2020-01-01T00:00:00Z\",\n  end_tweets = \"2020-05-01T00:00:00Z\",\n  n = 1000\n)\n\nWhile here we are collecting tweets by users “jack” and “cbarrrie” over the period January 1, 2020 to January 5, 2020 containing the words “twitter” or “social.”\nNote that the “AND” operator is implicit when specifying more than one character string in the query. See here for information on building queries for search tweets. Thus, when searching for all elements of a character string, a call may look like:\n\nget_all_tweets(\n  query = c(\"twitter social\"),\n  users = c(\"cbarrie\", \"jack\"),\n  start_tweets = \"2020-01-01T00:00:00Z\",\n  end_tweets = \"2020-05-01T00:00:00Z\",\n  n = 1000\n)\n\n, which will capture tweets containing both the words “twitter” and “social.” The same logics apply for hashtag queries.\nWhereas if we specify our query as separate elements of a character vector like this:\n\nget_all_tweets(\n  query = c(\"twitter\", \"social\"),\n  users = c(\"cbarrie\", \"jack\"),\n  start_tweets = \"2020-01-01T00:00:00Z\",\n  end_tweets = \"2020-05-01T00:00:00Z\",\n  n = 1000\n)\n\n, this will be capturing tweets by users “cbarrie” or “jack” containing the words “twitter” or social.\nFinally, we may wish to query an exact phrase. To do so, we can either input the phrase in escape quotes, e.g., query =\"\\\"Black Lives Matter\\\"\" or we can use the optional parameter exact_phrase = T to search for tweets containing the exact phrase string:\n\ntweets <-\n  get_all_tweets(\n    query = \"#BlackLivesMatter\",\n    start_tweets = \"2020-01-01T00:00:00Z\",\n    end_tweets = \"2020-01-05T00:00:00Z\",\n    file = \"blmtweets\",\n    data_path = \"data/\",\n    n = 1000000,\n  )\n\nHere, we are collecting tweets containing a hashtag related to the Black Lives Matter movement over the period January 1, 2020 to January 5, 2020.\nAnd the Twitter API is, of course, not the only API out there!\n\nOther APIs (R packages)\n\nhttps://cran.r-project.org/web/packages/manifestoR/index.html\nhttps://cran.r-project.org/web/packages/vkR/vkR.pdf\nhttps://cran.r-project.org/web/packages/tuber/index.html\n\n\n\nRehydrating tweets\nAs noted above, tweets are also often shared in the form of “IDs.” This is the case for the article by Barrie and Frey (2021). You can see some of the original tweet IDs we used for this article here.\nWe would then “hydrate” these tweets as follows using the academictwitteR package.\nI have provided a sample of these IDs for this worksheet, which you can consult yourselves.\n\ntweet_IDs <- readRDS(\"data/wm_IDs_samp.rds\")\nhead(tweet_IDs)\n\n[1] \"823360655835168640\" \"822727580923154432\" \"822953780127862784\"\n[4] \"823136522199253120\" \"821584199949942784\" \"822868286979342208\"\n\n\nAnd you can download these tweets from online to your own machines as follows:\n\ntweet_IDs <- readRDS(gzcon(url(\"https://github.com/cjbarrie/CS-ED/blob/main/data/wm_IDs_samp.rds?raw=true\")))\n\nThen we would rehydrate these tweets as follows using the academictwitteR package:\n\n#throws an error still for some IDs (looking into this)\nhydrated_tweets <- hydrate_tweets(tweet_IDs, errors = T,\n                                  data_path = \"data/hydrated_tweets/\")\n\nIf you can’t get access to an Academic Research Product Track API account, there are other ways still of getting the raw tweet data from publicly shared tweet IDs. One is the “hydrator” desktop tool made by organization Documenting the Now.\nYou can see more of their work—and download the hydrator tool here. There you will also find publicly available datasets of tweet IDs that you can hydrate yourselves.\nFor the article by Barrie and Frey (2021), they inferred the ideological position of individual users based on who they followed on Twitter. We won’t do this for the Women’s March IDs. But you can follow the steps below in order to estimate the ideology of my own Twitter account “@cbarrie”.\nFirst we need to get the my user ID (the unique set of integers that identifies an account). We can do this using academictwitteR or by using https://tweeterid.com/.\n\n\nLoading required package: R2WinBUGS\n\n\nLoading required package: coda\n\n\nLoading required package: boot\n\n\n##\n## tweetscores: tools for the analysis of Twitter data\n\n\n## Pablo Barbera (USC)\n\n\n## www.tweetscores.com\n##\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(academictwitteR)\ndevtools::install_github(\"pablobarbera/twitter_ideology/pkg/tweetscores\")\nlibrary(tweetscores)\n#use https://tweeterid.com/ to convert your username to an ID\n#OR\n#get user @cbarrie\nget_user_id(\"cbarrie\")\ncjb_ID <- \"95226101\"\n\nNow we can get the IDs of the accounts I follow and have a look at them:\n\n\n\n\n#find who he's following\nuserfwing <- get_user_following(cjb_ID)\n#get their IDs if you wanted to further analysis on the following network of @cbarrie\nids <- userfwing$id\nhead(ids)\n\n\n\n[1] \"1451947528333762562\" \"326311946\"           \"2458351124\"         \n[4] \"29943199\"            \"124852105\"           \"111456333\"          \n\n\nWe can then have a look at the public metrics (number of followers, tweets etc.) of the accounts I follow. And we could visualize these if we wished.\n\n#get the user metrics of whom @cbarrie is following\nuserfwing_metrics <- userfwing$public_metrics\n\n#plot \nuserfwing_metrics %>%\n  ggplot() +\n  geom_histogram(aes(log(followers_count)),\n                 binwidth = .5)\n\n\n\n\nThen finally we can use the tweetscores package to estimate the ideological position of the user in question. Though note that I only follow 2 of the relevant elites stored in this package so the estimates might not be accurate!\n\nresults <- estimateIdeology(\"cbarrie\", ids)\n\nplot(results)\n\n\n\nWarning: Ignoring unknown parameters: width\n\n\n\n\n\nAnd that is, essentially, how the ideology estimates were calculated in Barrie and Frey (2021) (except for a lot more people…!).\n\n\n\n\nBarrie, Christopher, and Arun Frey. 2021. “Faces in the Crowd: Twitter as Alternative to Protest Surveys.” Edited by Barbara Guidi. PLOS ONE 16 (11): e0259972. https://doi.org/10.1371/journal.pone.0259972."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allcott, Hunt, Luca Braghieri, Sarah Eichmeyer, and Matthew Gentzkow.\n2020. “The Welfare Effects of Social Media.” American\nEconomic Review 110 (3): 629676. https://doi.org/10.1257/aer.20190658.\n\n\nAllen, Jennifer, Baird Howland, Markus Mobius, David Rothschild, and\nDuncan J. Watts. 2020. “Evaluating the Fake News Problem at the\nScale of the Information Ecosystem.” Science Advances 6\n(14): eaay3539. https://doi.org/10.1126/sciadv.aay3539.\n\n\nBail, Chris. 2021. “Breaking the Social Media Prism.” In.\nPrinceton University Press.\n\n\nBail, Christopher A., Brian Guay, Emily Maloney, Aidan Combs, D.\nSunshine Hillygus, Friedolin Merhout, Deen Freelon, and Alexander\nVolfovsky. 2019. “Assessing the Russian Internet Research\nAgency’s Impact on the Political Attitudes and Behaviors of\nAmerican Twitter Users in Late 2017.” Proceedings of the\nNational Academy of Sciences 117 (1): 243–50. https://doi.org/10.1073/pnas.1906420116.\n\n\nBakshy, Eytan, Solomon Messing, and Lada A. Adamic. 2015.\n“Exposure to Ideologically Diverse News and Opinion on\nFacebook.” Science 348 (6239): 1130–32. https://doi.org/10.1126/science.aaa1160.\n\n\nBarrie, Christopher, and Arun Frey. 2021. “Faces in the Crowd:\nTwitter as Alternative to Protest Surveys.” Edited by Barbara\nGuidi. PLOS ONE 16 (11): e0259972. https://doi.org/10.1371/journal.pone.0259972.\n\n\nChen, M. Keith, and Ryne Rohla. 2018. “The Effect of Partisanship\nand Political Advertising on Close Family Ties.” Science\n360 (6392): 1020–24. https://doi.org/10.1126/science.aaq1433.\n\n\nChen, Wen, Diogo Pacheco, Kai-Cheng Yang, and Filippo Menczer. 2021.\n“Neutral Bots Probe Political Bias on Social Media.”\nNature Communications 12 (1). https://doi.org/10.1038/s41467-021-25738-6.\n\n\nConover, Michael, Jacob Ratkiewicz, Matthew Francisco, Bruno Goncalves,\nFilippo Menczer, and Alessandro Flammini. 2011. “Political\nPolarization on Twitter.” Proceedings of the International\nAAAI Conference on Web and Social Media 5 (1): 89–96. https://ojs.aaai.org/index.php/ICWSM/article/view/14126.\n\n\nFlaxman, Seth, Sharad Goel, and Justin M. Rao. 2016. “Filter\nBubbles, Echo Chambers, and Online News Consumption.” Public\nOpinion Quarterly 80 (S1): 298–320. https://doi.org/10.1093/poq/nfw006.\n\n\nFletcher, Richard, Craig T. Robertson, and Rasmus Kleis Nielsen. 2021.\n“How Many People Live in Politically Partisan Online News Echo\nChambers in Different Countries?” Journal of Quantitative\nDescription: Digital Media 1 (August). https://doi.org/10.51685/jqd.2021.020.\n\n\nGonzález-Bailón, Sandra, and Manlio De Domenico. 2021. “Bots Are\nLess Central Than Verified Accounts During Contentious Political\nEvents.” Proceedings of the National Academy of Sciences\n118 (11). https://doi.org/10.1073/pnas.2013443118.\n\n\nGrinberg, Nir, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson,\nand David Lazer. 2019. “Fake News on Twitter During the 2016 U.S.\nPresidential Election.” Science 363 (6425): 374–78. https://doi.org/10.1126/science.aau2706.\n\n\nGuess, Andrew M. 2021. “(Almost) Everything in Moderation: New\nEvidence on Americans’ Online Media Diets.” American Journal\nof Political Science 65 (4): 1007–22. https://doi.org/10.1111/ajps.12589.\n\n\nGuess, Andrew M., Brendan Nyhan, and Jason Reifler. 2020.\n“Exposure to Untrustworthy Websites in the 2016 US\nElection.” Nature Human Behaviour 4 (5): 472–80. https://doi.org/10.1038/s41562-020-0833-x.\n\n\nGuess, Andrew, Jonathan Nagler, and Joshua Tucker. 2019. “Less\nThan You Think: Prevalence and Predictors of Fake News Dissemination on\nFacebook.” Science Advances 5 (1). https://doi.org/10.1126/sciadv.aau4586.\n\n\nHalberstam, Yosh, and Brian Knight. 2016. “Homophily, Group Size,\nand the Diffusion of Political Information in Social Networks: Evidence\nfrom Twitter.” Journal of Public Economics 143\n(November): 73–88. https://doi.org/10.1016/j.jpubeco.2016.08.011.\n\n\nJuul, Jonas L., and Johan Ugander. 2021. “Comparing Information\nDiffusion Mechanisms by Matching on Cascade Size.”\nProceedings of the National Academy of Sciences 118 (46). https://doi.org/10.1073/pnas.2100786118.\n\n\nLazer, David M. J., Alex Pentland, Duncan J. Watts, Sinan Aral, Susan\nAthey, Noshir Contractor, Deen Freelon, et al. 2020.\n“Computational Social Science: Obstacles and\nOpportunities.” Science 369 (6507): 1060–62. https://doi.org/10.1126/science.aaz8170.\n\n\nLazer, David, Eszter Hargittai, Deen Freelon, Sandra Gonzalez-Bailon,\nKevin Munger, Katherine Ognyanova, and Jason Radford. 2021.\n“Meaningful Measures of Human Society in the Twenty-First\nCentury.” Nature 595 (7866): 189–96. https://doi.org/10.1038/s41586-021-03660-7.\n\n\nLedwich, Mark, and Anna Zaitsev. 2019. “Algorithmic Extremism:\nExamining YouTube’s Rabbit Hole of Radicalization.” https://doi.org/10.48550/ARXIV.1912.11211.\n\n\nLevy, Ro’ee. 2021. “Social Media, News Consumption, and\nPolarization: Evidence from a Field Experiment.” American\nEconomic Review 111 (3): 831870. https://doi.org/10.1257/aer.20191777.\n\n\nMosleh, Mohsen, Cameron Martel, Dean Eckles, and David G. Rand. 2021.\n“Shared Partisanship Dramatically Increases Social Tie Formation\nin a Twitter Field Experiment.” Proceedings of the National\nAcademy of Sciences 118 (7). https://doi.org/10.1073/pnas.2022761118.\n\n\nSalganik, Matthew J. 2017. Bit by Bit:\nSocial Research in the Digital\nAge. Princeton, NJ.: Princeton University Press.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining\nwith R: A Tidy\nApproach. London: O’Reilly.\n\n\nSTIER, SEBASTIAN, FRANK MANGOLD, MICHAEL SCHARKOW, and JOHANNES BREUER.\n2021. “Post Post-Broadcast Democracy? News Exposure in the Age of\nOnline Intermediaries.” American Political Science\nReview 116 (2): 768–74. https://doi.org/10.1017/s0003055421001222.\n\n\nVosoughi, Soroush, Deb Roy, and Sinan Aral. 2018. “The Spread of\nTrue and False News Online.” Science 359 (6380):\n1146–51. https://doi.org/10.1126/science.aap9559.\n\n\nWaller, Isaac, and Ashton Anderson. 2021. “Quantifying Social\nOrganization and Political Polarization in Online Platforms.”\nNature 600 (7888): 264–68. https://doi.org/10.1038/s41586-021-04167-x.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for\nData Science. London: O’Reilly Media.\n\n\nWing, Jeannette M. 2006. “Computational Thinking.”\nCommunications of the ACM 49 (3): 33–35."
  }
]